{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecad8b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:20:41.199765Z",
     "iopub.status.busy": "2023-01-23T13:20:41.199348Z",
     "iopub.status.idle": "2023-01-23T13:20:41.210901Z",
     "shell.execute_reply": "2023-01-23T13:20:41.210091Z"
    },
    "papermill": {
     "duration": 0.020225,
     "end_time": "2023-01-23T13:20:41.213059",
     "exception": false,
     "start_time": "2023-01-23T13:20:41.192834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_flag = False\n",
    "\n",
    "# Balance of type weighting タイプごとの重み付けバランス\n",
    "# 0:clicks 1:carts 2:orders\n",
    "\n",
    "type_weight = {0: 1, 1: 5, 2: 3}\n",
    "type_weight2 = {0:0.5, 1:9, 2:0.5}\n",
    "\n",
    "VER = 7\n",
    "\n",
    "# candidateの上位何件まで保存するか\n",
    "Ntop1 = 30\n",
    "Ntop2 = 30\n",
    "Ntop3 = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362f0c2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:20:41.221374Z",
     "iopub.status.busy": "2023-01-23T13:20:41.221080Z",
     "iopub.status.idle": "2023-01-23T13:20:43.810989Z",
     "shell.execute_reply": "2023-01-23T13:20:43.809329Z"
    },
    "papermill": {
     "duration": 2.596486,
     "end_time": "2023-01-23T13:20:43.813255",
     "exception": false,
     "start_time": "2023-01-23T13:20:41.216769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 21.10.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21298b69",
   "metadata": {
    "papermill": {
     "duration": 0.003465,
     "end_time": "2023-01-23T13:20:43.820420",
     "exception": false,
     "start_time": "2023-01-23T13:20:43.816955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute Three Co-visitation Matrices with RAPIDS\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405039ce",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-23T13:20:43.828669Z",
     "iopub.status.busy": "2023-01-23T13:20:43.828368Z",
     "iopub.status.idle": "2023-01-23T13:21:43.725100Z",
     "shell.execute_reply": "2023-01-23T13:21:43.724051Z"
    },
    "papermill": {
     "duration": 59.908094,
     "end_time": "2023-01-23T13:21:43.731999",
     "exception": false,
     "start_time": "2023-01-23T13:20:43.823905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n",
      "CPU times: user 44.6 s, sys: 8.48 s, total: 53.1 s\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "min_ts = 1659304800\n",
    "if valid_flag:\n",
    "    files = glob.glob('../input/otto-validation/*_parquet/*')\n",
    "    max_ts = 1661723996\n",
    "else:\n",
    "    files = glob.glob('/kaggle/input/otto-chunk-data-inparquet-format/*_parquet/*')\n",
    "    max_ts = 1662328791\n",
    "\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667d554",
   "metadata": {
    "papermill": {
     "duration": 0.003393,
     "end_time": "2023-01-23T13:21:43.739166",
     "exception": false,
     "start_time": "2023-01-23T13:21:43.735773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93552dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:21:43.747911Z",
     "iopub.status.busy": "2023-01-23T13:21:43.747615Z",
     "iopub.status.idle": "2023-01-23T13:25:06.042836Z",
     "shell.execute_reply": "2023-01-23T13:25:06.041782Z"
    },
    "papermill": {
     "duration": 202.302225,
     "end_time": "2023-01-23T13:25:06.044946",
     "exception": false,
     "start_time": "2023-01-23T13:21:43.742721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 10s, sys: 1min 10s, total: 3min 20s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "#DISK_PIECES = 16\n",
    "DISK_PIECES = 4\n",
    "\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "            \n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "        \n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP Ntop\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<Ntop1].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_{Ntop1}_carts_orders_v{VER}_{PART}_ver0.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc36b6",
   "metadata": {
    "papermill": {
     "duration": 0.010057,
     "end_time": "2023-01-23T13:25:06.065611",
     "exception": false,
     "start_time": "2023-01-23T13:25:06.055554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1-2) \"Carts Orders\" Co-visitation Matrix - Type Weighted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48adcfea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:25:06.087363Z",
     "iopub.status.busy": "2023-01-23T13:25:06.087065Z",
     "iopub.status.idle": "2023-01-23T13:28:26.951928Z",
     "shell.execute_reply": "2023-01-23T13:28:26.950105Z"
    },
    "papermill": {
     "duration": 200.879034,
     "end_time": "2023-01-23T13:28:26.954718",
     "exception": false,
     "start_time": "2023-01-23T13:25:06.075684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 9s, sys: 1min 10s, total: 3min 20s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "#DISK_PIECES = 16\n",
    "DISK_PIECES = 4\n",
    "\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            # 違うのここだけ\n",
    "            df['wgt'] = df.type_y.map(type_weight2)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "            \n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "        \n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP Ntop\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<Ntop1].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_{Ntop1}_carts_orders_v{VER}_{PART}_ver1.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f31ae",
   "metadata": {
    "papermill": {
     "duration": 0.016712,
     "end_time": "2023-01-23T13:28:26.989210",
     "exception": false,
     "start_time": "2023-01-23T13:28:26.972498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0034faa6",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-23T13:28:27.024212Z",
     "iopub.status.busy": "2023-01-23T13:28:27.023916Z",
     "iopub.status.idle": "2023-01-23T13:28:59.530388Z",
     "shell.execute_reply": "2023-01-23T13:28:59.529384Z"
    },
    "papermill": {
     "duration": 32.546493,
     "end_time": "2023-01-23T13:28:59.552534",
     "exception": false,
     "start_time": "2023-01-23T13:28:27.006041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 21.9 s, sys: 10 s, total: 32 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "#DISK_PIECES = 4\n",
    "DISK_PIECES = 1\n",
    "\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            \n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            \n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    \n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP Ntop\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<Ntop2].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_{Ntop2}_buy2buy_v{VER}_{PART}_ver0.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63ec46",
   "metadata": {
    "papermill": {
     "duration": 0.027272,
     "end_time": "2023-01-23T13:28:59.599679",
     "exception": false,
     "start_time": "2023-01-23T13:28:59.572407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2-2) \"Buy2Buy\" Co-visitation Matrix, 7days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e626df7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:28:59.638668Z",
     "iopub.status.busy": "2023-01-23T13:28:59.638344Z",
     "iopub.status.idle": "2023-01-23T13:29:27.986495Z",
     "shell.execute_reply": "2023-01-23T13:29:27.984141Z"
    },
    "papermill": {
     "duration": 28.370493,
     "end_time": "2023-01-23T13:29:27.989045",
     "exception": false,
     "start_time": "2023-01-23T13:28:59.618552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 19.2 s, sys: 9.11 s, total: 28.3 s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "#DISK_PIECES = 4\n",
    "DISK_PIECES = 1\n",
    "\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            \n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            \n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 7 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            \n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    \n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP Ntop\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<Ntop2].drop('n',axis=1)\n",
    "    \n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_{Ntop2}_buy2buy_v{VER}_{PART}_ver1.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f3578",
   "metadata": {
    "papermill": {
     "duration": 0.020648,
     "end_time": "2023-01-23T13:29:28.030680",
     "exception": false,
     "start_time": "2023-01-23T13:29:28.010032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01f5cff",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-23T13:29:28.072614Z",
     "iopub.status.busy": "2023-01-23T13:29:28.072269Z",
     "iopub.status.idle": "2023-01-23T13:32:47.808401Z",
     "shell.execute_reply": "2023-01-23T13:32:47.807033Z"
    },
    "papermill": {
     "duration": 199.75965,
     "end_time": "2023-01-23T13:32:47.810536",
     "exception": false,
     "start_time": "2023-01-23T13:29:28.050886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 2min 9s, sys: 1min 9s, total: 3min 18s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "#DISK_PIECES = 16\n",
    "DISK_PIECES = 4\n",
    "\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            \n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            \n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            \n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - min_ts)/(max_ts-min_ts)\n",
    "\n",
    "            \n",
    "            # 1659304800 : minimum timestamp\n",
    "            # 1662328791 : maximum timestamp\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            \n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    \n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    \n",
    "    # SAVE TOP Ntop\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<Ntop3].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_{Ntop3}_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d79c97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-23T13:32:47.868591Z",
     "iopub.status.busy": "2023-01-23T13:32:47.868260Z",
     "iopub.status.idle": "2023-01-23T13:32:48.032667Z",
     "shell.execute_reply": "2023-01-23T13:32:48.031703Z"
    },
    "papermill": {
     "duration": 0.194715,
     "end_time": "2023-01-23T13:32:48.035088",
     "exception": false,
     "start_time": "2023-01-23T13:32:47.840373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571db0c",
   "metadata": {
    "papermill": {
     "duration": 0.027234,
     "end_time": "2023-01-23T13:32:48.089418",
     "exception": false,
     "start_time": "2023-01-23T13:32:48.062184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 736.02587,
   "end_time": "2023-01-23T13:32:49.439169",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-23T13:20:33.413299",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
