{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1673272049837,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1673272053656,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8222,
     "status": "ok",
     "timestamp": 1673272061875,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train.parquet')\n",
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_20.parquet')\n",
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "\n",
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_20_old.parquet')\n",
    "\n",
    "#train20 = pd.read_parquet('/content/drive/MyDrive/output/otto/train_20.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1673272061875,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "#DEBUG_MODE = True\n",
    "\n",
    "OPTUNA_FLAG = False\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL = ['session','aid']\n",
    "\n",
    "TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "#TYPE_MODE = 'orders'\n",
    "IGNORE_COL += ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1673272061876,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    df['session'] = df['session'].astype('int32')\n",
    "    df['aid'] = df['aid'].astype('int32')\n",
    "    df['score_click'] = df['score_click'].astype('float32')\n",
    "    df['score_cart'] = df['score_cart'].astype('float32')\n",
    "    df['score_buy'] = df['score_buy'].astype('float32')\n",
    "    df['score_click_only'] = df['score_click_only'].astype('float32')\n",
    "    df['score_cart_only'] = df['score_cart_only'].astype('float32')\n",
    "    df['score_buy_only'] = df['score_buy_only'].astype('float32')\n",
    "    df['session_action_count'] = df['session_action_count'].astype('int16')\n",
    "    df['session_click_count'] = df['session_click_count'].astype('int16')\n",
    "    df['session_cart_count'] = df['session_cart_count'].astype('int16')\n",
    "    df['session_order_count'] = df['session_order_count'].astype('int16')\n",
    "    df['session_type_mean'] = df['session_type_mean'].astype('float32')\n",
    "    \n",
    "    click_topn_list = [10, 20]\n",
    "    for i in click_topn_list:\n",
    "        df[f'n_clicks_{i}'] = df[f'n_clicks_{i}'].astype('int8')\n",
    "\n",
    "    df['n_carts'] = df['n_carts'].astype('int8')\n",
    "    df['n_buys'] = df['n_buys'].astype('int8')\n",
    "    df['clicks_count'] = df['clicks_count'].astype('int32')\n",
    "    df['carts_count'] = df['carts_count'].astype('int16')\n",
    "    df['orders_count'] = df['orders_count'].astype('int16')\n",
    "    return df\n",
    "\n",
    "# topn件だけを使う\n",
    "def use_top_n(n, df):\n",
    "    df = df.query(f'score_click >= -1 or score_cart >= -1 or score_buy >= -1 or (-1 < n_clicks_20 and n_clicks_20<{n}) or (-1 < n_carts and n_carts<{n}) or (-1 < n_buys and n_buys<{n})')\n",
    "    return df\n",
    "\n",
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df):\n",
    "    true_df = df.groupby('session')[target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1673272061876,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "KvbDeqvHbMn8"
   },
   "outputs": [],
   "source": [
    "def join_session_features(df):\n",
    "    session_df = pd.read_parquet('/content/drive/MyDrive/output/otto/valid_session_features.parquet')\n",
    "    week_list = ['4weeks', '2weeks', '1week']\n",
    "    session_df['session'] = session_df['session'].astype('int32')\n",
    "    for i in week_list:\n",
    "        session_df[f'session_action_count_{i}'] = session_df[f'session_action_count_{i}'].astype('int16')\n",
    "        session_df[f'session_click_count_{i}'] = session_df[f'session_click_count_{i}'].astype('int16')\n",
    "        session_df[f'session_cart_count_{i}'] = session_df[f'session_cart_count_{i}'].astype('int16')\n",
    "        session_df[f'session_order_count_{i}'] = session_df[f'session_order_count_{i}'].astype('int16')\n",
    "        session_df[f'session_type_mean_{i}'] = session_df[f'session_type_mean_{i}'].astype('float32')\n",
    "        session_df[f'session_click_rate_{i}'] = session_df[f'session_click_rate_{i}'].astype('float32')\n",
    "        session_df[f'session_cart_rate_{i}'] = session_df[f'session_cart_rate_{i}'].astype('float32')\n",
    "        session_df[f'session_order_rate_{i}'] = session_df[f'session_order_rate_{i}'].astype('float32')\n",
    "\n",
    "    #remove_col = ['session_action_count', 'session_click_count', 'session_cart_count', 'session_order_count', 'session_type_mean']\n",
    "    #df.drop(remove_col , axis=1)\n",
    "    df = df.merge(session_df, 'left', 'session')\n",
    "    del session_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1673272061876,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "TT-tm_hPbPpA"
   },
   "outputs": [],
   "source": [
    "def join_aid_features(df):\n",
    "    aid_df = pd.read_parquet(f'{base_path}/output/otto/valid_aid_features.parquet')\n",
    "    #week_list = ['4weeks', '3weeks', '2weeks', '1week']\n",
    "    week_list = ['4weeks', '2weeks', '1week']\n",
    "    aid_df['aid'] = aid_df['aid'].astype('int32')\n",
    "    for i in week_list:\n",
    "        aid_df[f'clicks_count_{i}'] = aid_df[f'clicks_count_{i}'].astype('int32')\n",
    "        aid_df[f'carts_count_{i}'] = aid_df[f'carts_count_{i}'].astype('int16')\n",
    "        aid_df[f'orders_count_{i}'] = aid_df[f'orders_count_{i}'].astype('int16')\n",
    "        aid_df[f'clicks_rank_{i}'] = aid_df[f'clicks_rank_{i}'].astype('int32')\n",
    "        aid_df[f'carts_rank_{i}'] = aid_df[f'carts_rank_{i}'].astype('int32')\n",
    "        aid_df[f'orders_rank_{i}'] = aid_df[f'orders_rank_{i}'].astype('int32')\n",
    "        for j in ['clicks', 'carts', 'orders']:\n",
    "            #for k in [2,3,4]:\n",
    "            for k in [2,4]:\n",
    "                aid_df[f'aid_{j}_count_rate_1_{k}'] = aid_df[f'aid_{j}_count_rate_1_{k}'].astype('float32')\n",
    "\n",
    "    remove_col = ['clicks_rank', 'carts_rank', 'orders_rank', 'clicks_count', 'carts_count', 'orders_count']\n",
    "    #remove_col = ['clicks_rank_1week', 'carts_rank_1week', 'orders_rank_1week', 'clicks_count_1week', 'carts_count_1week', 'orders_count_1week']\n",
    "    #df.drop(remove_col , axis=1)\n",
    "    df = df.merge(aid_df, 'left', 'aid')\n",
    "    del aid_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4951,
     "status": "ok",
     "timestamp": 1673272066805,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = reduce_memory(train)\n",
    "train = use_top_n(50, train)\n",
    "train = remove_negative_session(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1673272087457,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yyec9GHZypQ5",
    "outputId": "c573110b-b258-42e3-f78c-6acb7770b2dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[target].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1673272087856,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "RGToJvwN2dxO",
    "outputId": "7facba5d-97b9-4291-9b28-e1d7912847c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013401377389554573"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 48878,
     "status": "ok",
     "timestamp": 1673272136732,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "_V01FZbPcACE"
   },
   "outputs": [],
   "source": [
    "#train = join_session_features(train)\n",
    "train = join_aid_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1673272136732,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XWIbhd2JjWM7",
    "outputId": "de98a445-8702-44ac-ffdf-a88ecdbdfa9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>score_click</th>\n",
       "      <th>score_cart</th>\n",
       "      <th>score_buy</th>\n",
       "      <th>score_click_only</th>\n",
       "      <th>score_cart_only</th>\n",
       "      <th>score_buy_only</th>\n",
       "      <th>session_action_count</th>\n",
       "      <th>session_click_count</th>\n",
       "      <th>session_cart_count</th>\n",
       "      <th>session_order_count</th>\n",
       "      <th>session_type_mean</th>\n",
       "      <th>n_clicks_10</th>\n",
       "      <th>n_clicks_20</th>\n",
       "      <th>n_carts</th>\n",
       "      <th>n_buys</th>\n",
       "      <th>clicks_rank</th>\n",
       "      <th>carts_rank</th>\n",
       "      <th>orders_rank</th>\n",
       "      <th>clicks_count</th>\n",
       "      <th>carts_count</th>\n",
       "      <th>orders_count</th>\n",
       "      <th>y_clicks</th>\n",
       "      <th>y_carts</th>\n",
       "      <th>y_orders</th>\n",
       "      <th>clicks_count_4weeks</th>\n",
       "      <th>carts_count_4weeks</th>\n",
       "      <th>orders_count_4weeks</th>\n",
       "      <th>clicks_rank_4weeks</th>\n",
       "      <th>carts_rank_4weeks</th>\n",
       "      <th>orders_rank_4weeks</th>\n",
       "      <th>clicks_count_2weeks</th>\n",
       "      <th>carts_count_2weeks</th>\n",
       "      <th>orders_count_2weeks</th>\n",
       "      <th>clicks_rank_2weeks</th>\n",
       "      <th>carts_rank_2weeks</th>\n",
       "      <th>orders_rank_2weeks</th>\n",
       "      <th>clicks_count_1week</th>\n",
       "      <th>carts_count_1week</th>\n",
       "      <th>orders_count_1week</th>\n",
       "      <th>clicks_rank_1week</th>\n",
       "      <th>carts_rank_1week</th>\n",
       "      <th>orders_rank_1week</th>\n",
       "      <th>aid_clicks_count_rate_1_2</th>\n",
       "      <th>aid_clicks_count_rate_1_4</th>\n",
       "      <th>aid_carts_count_rate_1_2</th>\n",
       "      <th>aid_carts_count_rate_1_4</th>\n",
       "      <th>aid_orders_count_rate_1_2</th>\n",
       "      <th>aid_orders_count_rate_1_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098529</td>\n",
       "      <td>1105029</td>\n",
       "      <td>0.071773</td>\n",
       "      <td>0.071773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>207743</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>126128</td>\n",
       "      <td>344989</td>\n",
       "      <td>527328</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>122647</td>\n",
       "      <td>393631</td>\n",
       "      <td>331822</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231744</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529</td>\n",
       "      <td>459126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>72021</td>\n",
       "      <td>123811</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>455</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>56855</td>\n",
       "      <td>159598</td>\n",
       "      <td>539614</td>\n",
       "      <td>199</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>47159</td>\n",
       "      <td>141798</td>\n",
       "      <td>-1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71369</td>\n",
       "      <td>125761</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.080402</td>\n",
       "      <td>0.035165</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098529</td>\n",
       "      <td>1339838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1645</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>12332</td>\n",
       "      <td>43351</td>\n",
       "      <td>-1</td>\n",
       "      <td>631</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>11754</td>\n",
       "      <td>46034</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098529</td>\n",
       "      <td>1544564</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>34053</td>\n",
       "      <td>57613</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>820</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>29691</td>\n",
       "      <td>59118</td>\n",
       "      <td>-1</td>\n",
       "      <td>287</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>31556</td>\n",
       "      <td>69451</td>\n",
       "      <td>-1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34807</td>\n",
       "      <td>60893</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.111498</td>\n",
       "      <td>0.039024</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098529</td>\n",
       "      <td>217742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>131217</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>184</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>135883</td>\n",
       "      <td>288970</td>\n",
       "      <td>292434</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>89687</td>\n",
       "      <td>378158</td>\n",
       "      <td>280473</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126394</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.048913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75569466</th>\n",
       "      <td>12899778</td>\n",
       "      <td>162064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>45</td>\n",
       "      <td>901</td>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>455</td>\n",
       "      <td>192</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3169</td>\n",
       "      <td>2768</td>\n",
       "      <td>1309</td>\n",
       "      <td>4965</td>\n",
       "      <td>85</td>\n",
       "      <td>36</td>\n",
       "      <td>1950</td>\n",
       "      <td>1220</td>\n",
       "      <td>523</td>\n",
       "      <td>2230</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>455</td>\n",
       "      <td>192</td>\n",
       "      <td>31</td>\n",
       "      <td>896</td>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.143578</td>\n",
       "      <td>0.157377</td>\n",
       "      <td>0.069364</td>\n",
       "      <td>0.059273</td>\n",
       "      <td>0.023682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75569467</th>\n",
       "      <td>12899778</td>\n",
       "      <td>631899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>46</td>\n",
       "      <td>108</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>1355</td>\n",
       "      <td>145</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17258</td>\n",
       "      <td>2216</td>\n",
       "      <td>973</td>\n",
       "      <td>266</td>\n",
       "      <td>140</td>\n",
       "      <td>83</td>\n",
       "      <td>9260</td>\n",
       "      <td>1127</td>\n",
       "      <td>495</td>\n",
       "      <td>124</td>\n",
       "      <td>64</td>\n",
       "      <td>21</td>\n",
       "      <td>1355</td>\n",
       "      <td>145</td>\n",
       "      <td>31</td>\n",
       "      <td>108</td>\n",
       "      <td>84</td>\n",
       "      <td>31</td>\n",
       "      <td>0.146328</td>\n",
       "      <td>0.078514</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.062626</td>\n",
       "      <td>0.031860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75569468</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1436280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>47</td>\n",
       "      <td>151</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>1219</td>\n",
       "      <td>145</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25897</td>\n",
       "      <td>2999</td>\n",
       "      <td>536</td>\n",
       "      <td>110</td>\n",
       "      <td>74</td>\n",
       "      <td>273</td>\n",
       "      <td>12329</td>\n",
       "      <td>1559</td>\n",
       "      <td>313</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>82</td>\n",
       "      <td>1219</td>\n",
       "      <td>145</td>\n",
       "      <td>30</td>\n",
       "      <td>151</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>0.098873</td>\n",
       "      <td>0.047071</td>\n",
       "      <td>0.093008</td>\n",
       "      <td>0.048349</td>\n",
       "      <td>0.095847</td>\n",
       "      <td>0.055970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75569469</th>\n",
       "      <td>12899778</td>\n",
       "      <td>954951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>48</td>\n",
       "      <td>109</td>\n",
       "      <td>117</td>\n",
       "      <td>33</td>\n",
       "      <td>1353</td>\n",
       "      <td>123</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17585</td>\n",
       "      <td>1660</td>\n",
       "      <td>606</td>\n",
       "      <td>254</td>\n",
       "      <td>232</td>\n",
       "      <td>221</td>\n",
       "      <td>8811</td>\n",
       "      <td>879</td>\n",
       "      <td>315</td>\n",
       "      <td>144</td>\n",
       "      <td>113</td>\n",
       "      <td>81</td>\n",
       "      <td>1353</td>\n",
       "      <td>123</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>117</td>\n",
       "      <td>33</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>0.076941</td>\n",
       "      <td>0.139932</td>\n",
       "      <td>0.074096</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75569470</th>\n",
       "      <td>12899778</td>\n",
       "      <td>373490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>49</td>\n",
       "      <td>148</td>\n",
       "      <td>69</td>\n",
       "      <td>34</td>\n",
       "      <td>1238</td>\n",
       "      <td>158</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1785</td>\n",
       "      <td>217</td>\n",
       "      <td>51</td>\n",
       "      <td>11048</td>\n",
       "      <td>6990</td>\n",
       "      <td>11659</td>\n",
       "      <td>1372</td>\n",
       "      <td>167</td>\n",
       "      <td>32</td>\n",
       "      <td>3899</td>\n",
       "      <td>2291</td>\n",
       "      <td>5062</td>\n",
       "      <td>1238</td>\n",
       "      <td>158</td>\n",
       "      <td>30</td>\n",
       "      <td>148</td>\n",
       "      <td>69</td>\n",
       "      <td>34</td>\n",
       "      <td>0.902332</td>\n",
       "      <td>0.693557</td>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.728111</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75569471 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           session      aid  score_click  score_cart  score_buy  score_click_only  score_cart_only  score_buy_only  session_action_count  session_click_count  session_cart_count  session_order_count  session_type_mean  n_clicks_10  n_clicks_20  n_carts  n_buys  clicks_rank  carts_rank  orders_rank  clicks_count  carts_count  orders_count  y_clicks  y_carts  y_orders  clicks_count_4weeks  carts_count_4weeks  orders_count_4weeks  clicks_rank_4weeks  carts_rank_4weeks  orders_rank_4weeks  clicks_count_2weeks  carts_count_2weeks  orders_count_2weeks  clicks_rank_2weeks  carts_rank_2weeks  orders_rank_2weeks  clicks_count_1week  carts_count_1week  orders_count_1week  clicks_rank_1week  carts_rank_1week  orders_rank_1week  aid_clicks_count_rate_1_2  aid_clicks_count_rate_1_4  aid_carts_count_rate_1_2  aid_carts_count_rate_1_4  aid_orders_count_rate_1_2  aid_orders_count_rate_1_4\n",
       "0         11098529  1105029     0.071773    0.071773        0.0          0.071773              0.0             0.0                     1                    1                   0                    0                0.0           -1           -1       -1      -1       207743          -1           -1             5            0             0      True    False     False                  200                   5                    1              126128             344989              527328                   76                   2                    1              122647             393631              331822                   5                  0                   0             231744                -1                 -1                   0.065789                   0.025000                  0.000000                  0.000000                   0.000000                   0.000000\n",
       "1         11098529   459126          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN            0            0        0       3        72021      123811           -1            16            1             0     False    False     False                  455                  14                    1               56855             159598              539614                  199                   6                    0               47159             141798                  -1                  16                  1                   0              71369            125761                 -1                   0.080402                   0.035165                  0.166667                  0.071429                   0.000000                   0.000000\n",
       "2         11098529  1339838          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN            1            1        1       7           -1          -1           -1             0            0             0     False    False     False                 1645                  52                    0               12332              43351                  -1                  631                  19                    0               11754              46034                  -1                   0                  0                   0                 -1                -1                 -1                   0.000000                   0.000000                  0.000000                  0.000000                   0.000000                   0.000000\n",
       "3         11098529  1544564          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN            2            2        4      -1        34053       57613           -1            32            2             0     False    False     False                  820                  39                    0               29691              59118                  -1                  287                  13                    0               31556              69451                  -1                  32                  2                   0              34807             60893                 -1                   0.111498                   0.039024                  0.153846                  0.051282                   0.000000                   0.000000\n",
       "4         11098529   217742          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN            3            3        5      -1       131217          -1           -1             9            0             0     False    False     False                  184                   7                    2              135883             288970              292434                  107                   2                    1               89687             378158              280473                   9                  0                   0             126394                -1                 -1                   0.084112                   0.048913                  0.000000                  0.000000                   0.000000                   0.000000\n",
       "...            ...      ...          ...         ...        ...               ...              ...             ...                   ...                  ...                 ...                  ...                ...          ...          ...      ...     ...          ...         ...          ...           ...          ...           ...       ...      ...       ...                  ...                 ...                  ...                 ...                ...                 ...                  ...                 ...                  ...                 ...                ...                 ...                 ...                ...                 ...                ...               ...                ...                        ...                        ...                       ...                       ...                        ...                        ...\n",
       "75569466  12899778   162064          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN           -1           -1       -1      45          901          46           30           455          192            31     False    False     False                 3169                2768                 1309                4965                 85                  36                 1950                1220                  523                2230                 50                  17                 455                192                  31                896                46                 30                   0.233333                   0.143578                  0.157377                  0.069364                   0.059273                   0.023682\n",
       "75569467  12899778   631899          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN           -1           -1       -1      46          108          83           31          1355          145            31     False    False     False                17258                2216                  973                 266                140                  83                 9260                1127                  495                 124                 64                  21                1355                145                  31                108                84                 31                   0.146328                   0.078514                  0.128660                  0.065433                   0.062626                   0.031860\n",
       "75569468  12899778  1436280          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN           -1           -1       -1      47          151          82           32          1219          145            30     False    False     False                25897                2999                  536                 110                 74                 273                12329                1559                  313                  52                 31                  82                1219                145                  30                151                82                 32                   0.098873                   0.047071                  0.093008                  0.048349                   0.095847                   0.055970\n",
       "75569469  12899778   954951          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN           -1           -1       -1      48          109         117           33          1353          123            30     False    False     False                17585                1660                  606                 254                232                 221                 8811                 879                  315                 144                113                  81                1353                123                  30                109               117                 33                   0.153558                   0.076941                  0.139932                  0.074096                   0.095238                   0.049505\n",
       "75569470  12899778   373490          NaN         NaN        NaN               NaN              NaN             NaN                     0                    0                   0                    0                NaN           -1           -1       -1      49          148          69           34          1238          158            30     False    False     False                 1785                 217                   51               11048               6990               11659                 1372                 167                   32                3899               2291                5062                1238                158                  30                148                69                 34                   0.902332                   0.693557                  0.946108                  0.728111                   0.937500                   0.588235\n",
       "\n",
       "[75569471 rows x 50 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1673272136733,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eY1bGfqgM-Oi",
    "outputId": "72c04650-d9be-4060-8250-c08d1d91edb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session                        int32\n",
       "aid                            int32\n",
       "score_click                  float32\n",
       "score_cart                   float32\n",
       "score_buy                    float32\n",
       "score_click_only             float32\n",
       "score_cart_only              float32\n",
       "score_buy_only               float32\n",
       "session_action_count           int16\n",
       "session_click_count            int16\n",
       "session_cart_count             int16\n",
       "session_order_count            int16\n",
       "session_type_mean            float32\n",
       "n_clicks_10                     int8\n",
       "n_clicks_20                     int8\n",
       "n_carts                         int8\n",
       "n_buys                          int8\n",
       "clicks_rank                    int32\n",
       "carts_rank                     int32\n",
       "orders_rank                    int32\n",
       "clicks_count                   int32\n",
       "carts_count                    int16\n",
       "orders_count                   int16\n",
       "y_clicks                        bool\n",
       "y_carts                         bool\n",
       "y_orders                        bool\n",
       "clicks_count_4weeks            int32\n",
       "carts_count_4weeks             int16\n",
       "orders_count_4weeks            int16\n",
       "clicks_rank_4weeks             int32\n",
       "carts_rank_4weeks              int32\n",
       "orders_rank_4weeks             int32\n",
       "clicks_count_2weeks            int32\n",
       "carts_count_2weeks             int16\n",
       "orders_count_2weeks            int16\n",
       "clicks_rank_2weeks             int32\n",
       "carts_rank_2weeks              int32\n",
       "orders_rank_2weeks             int32\n",
       "clicks_count_1week             int32\n",
       "carts_count_1week              int16\n",
       "orders_count_1week             int16\n",
       "clicks_rank_1week              int32\n",
       "carts_rank_1week               int32\n",
       "orders_rank_1week              int32\n",
       "aid_clicks_count_rate_1_2    float32\n",
       "aid_clicks_count_rate_1_4    float32\n",
       "aid_carts_count_rate_1_2     float32\n",
       "aid_carts_count_rate_1_4     float32\n",
       "aid_orders_count_rate_1_2    float32\n",
       "aid_orders_count_rate_1_4    float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1673272136733,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1673272136734,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',  # Noneにした方がよさそう？\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}...')\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1673272136734,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "params =  {'objective': 'binary',\n",
    " 'metric': 'binary_logloss',\n",
    " 'boosting': 'gbdt',\n",
    " 'seed': 42,\n",
    " 'n_jobs': -1,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 6.595370151657238,\n",
    " 'lambda_l2': 1.0592737233474818e-08,\n",
    " 'num_leaves': 255,\n",
    " 'feature_fraction': 1.0,\n",
    " 'bagging_fraction': 0.9703737428957173,\n",
    " 'bagging_freq': 2,\n",
    " 'min_child_samples': 20,\n",
    " 'learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "e04ea3ed-ce62-439d-daad-34b0ee161e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0...\n",
      "before mean: 0.013403676775819653\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "[LightGBM] [Info] Number of positive: 810327, number of negative: 31602753\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.067044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9186\n",
      "[LightGBM] [Info] Number of data points in the train set: 32413080, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025000 -> initscore=-3.663562\n",
      "[LightGBM] [Info] Start training from score -3.663562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[5]\ttraining's binary_logloss: 0.0785937\tvalid_1's binary_logloss: 0.0531982\n",
      "[10]\ttraining's binary_logloss: 0.0726076\tvalid_1's binary_logloss: 0.0492853\n",
      "[15]\ttraining's binary_logloss: 0.0697347\tvalid_1's binary_logloss: 0.047359\n",
      "[20]\ttraining's binary_logloss: 0.0682343\tvalid_1's binary_logloss: 0.0463377\n",
      "[25]\ttraining's binary_logloss: 0.0674143\tvalid_1's binary_logloss: 0.0457751\n",
      "[30]\ttraining's binary_logloss: 0.0669486\tvalid_1's binary_logloss: 0.0454562\n",
      "[35]\ttraining's binary_logloss: 0.0666711\tvalid_1's binary_logloss: 0.0452667\n",
      "[40]\ttraining's binary_logloss: 0.0664997\tvalid_1's binary_logloss: 0.0451557\n",
      "[45]\ttraining's binary_logloss: 0.0663869\tvalid_1's binary_logloss: 0.0450868\n",
      "[50]\ttraining's binary_logloss: 0.0663081\tvalid_1's binary_logloss: 0.0450422\n",
      "[55]\ttraining's binary_logloss: 0.0662457\tvalid_1's binary_logloss: 0.0450103\n",
      "[60]\ttraining's binary_logloss: 0.066194\tvalid_1's binary_logloss: 0.0449891\n",
      "[65]\ttraining's binary_logloss: 0.0661543\tvalid_1's binary_logloss: 0.0449756\n",
      "[70]\ttraining's binary_logloss: 0.0661143\tvalid_1's binary_logloss: 0.0449631\n",
      "[75]\ttraining's binary_logloss: 0.0660757\tvalid_1's binary_logloss: 0.0449527\n",
      "[80]\ttraining's binary_logloss: 0.0660441\tvalid_1's binary_logloss: 0.0449471\n",
      "[85]\ttraining's binary_logloss: 0.066012\tvalid_1's binary_logloss: 0.0449406\n",
      "[90]\ttraining's binary_logloss: 0.065978\tvalid_1's binary_logloss: 0.0449331\n",
      "[95]\ttraining's binary_logloss: 0.0659498\tvalid_1's binary_logloss: 0.0449289\n",
      "[100]\ttraining's binary_logloss: 0.0659231\tvalid_1's binary_logloss: 0.0449254\n",
      "[105]\ttraining's binary_logloss: 0.0658946\tvalid_1's binary_logloss: 0.0449206\n",
      "[110]\ttraining's binary_logloss: 0.0658673\tvalid_1's binary_logloss: 0.0449173\n",
      "[115]\ttraining's binary_logloss: 0.0658386\tvalid_1's binary_logloss: 0.0449136\n",
      "[120]\ttraining's binary_logloss: 0.065814\tvalid_1's binary_logloss: 0.0449109\n",
      "[125]\ttraining's binary_logloss: 0.0657882\tvalid_1's binary_logloss: 0.0449098\n",
      "[130]\ttraining's binary_logloss: 0.0657621\tvalid_1's binary_logloss: 0.044907\n",
      "[135]\ttraining's binary_logloss: 0.0657414\tvalid_1's binary_logloss: 0.0449058\n",
      "[140]\ttraining's binary_logloss: 0.0657139\tvalid_1's binary_logloss: 0.044902\n",
      "[145]\ttraining's binary_logloss: 0.0656876\tvalid_1's binary_logloss: 0.044898\n",
      "[150]\ttraining's binary_logloss: 0.0656605\tvalid_1's binary_logloss: 0.0448946\n",
      "[155]\ttraining's binary_logloss: 0.0656368\tvalid_1's binary_logloss: 0.0448925\n",
      "[160]\ttraining's binary_logloss: 0.065614\tvalid_1's binary_logloss: 0.0448906\n",
      "[165]\ttraining's binary_logloss: 0.0655959\tvalid_1's binary_logloss: 0.0448894\n",
      "[170]\ttraining's binary_logloss: 0.065572\tvalid_1's binary_logloss: 0.0448866\n",
      "[175]\ttraining's binary_logloss: 0.0655454\tvalid_1's binary_logloss: 0.0448826\n",
      "[180]\ttraining's binary_logloss: 0.0655215\tvalid_1's binary_logloss: 0.0448819\n",
      "[185]\ttraining's binary_logloss: 0.0654964\tvalid_1's binary_logloss: 0.0448798\n",
      "[190]\ttraining's binary_logloss: 0.0654725\tvalid_1's binary_logloss: 0.0448766\n",
      "[195]\ttraining's binary_logloss: 0.0654481\tvalid_1's binary_logloss: 0.0448744\n",
      "[200]\ttraining's binary_logloss: 0.0654247\tvalid_1's binary_logloss: 0.044871\n",
      "[205]\ttraining's binary_logloss: 0.0654048\tvalid_1's binary_logloss: 0.0448709\n",
      "[210]\ttraining's binary_logloss: 0.065382\tvalid_1's binary_logloss: 0.0448708\n",
      "Early stopping, best iteration is:\n",
      "[201]\ttraining's binary_logloss: 0.0654197\tvalid_1's binary_logloss: 0.0448703\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1...\n",
      "before mean: 0.013402485596986362\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "[LightGBM] [Info] Number of positive: 810255, number of negative: 31599945\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.530468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9169\n",
      "[LightGBM] [Info] Number of data points in the train set: 32410200, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025000 -> initscore=-3.663562\n",
      "[LightGBM] [Info] Start training from score -3.663562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[5]\ttraining's binary_logloss: 0.0785879\tvalid_1's binary_logloss: 0.0531935\n",
      "[10]\ttraining's binary_logloss: 0.0726065\tvalid_1's binary_logloss: 0.049289\n",
      "[15]\ttraining's binary_logloss: 0.0697359\tvalid_1's binary_logloss: 0.0473605\n",
      "[20]\ttraining's binary_logloss: 0.0682345\tvalid_1's binary_logloss: 0.0463375\n",
      "[25]\ttraining's binary_logloss: 0.067415\tvalid_1's binary_logloss: 0.0457746\n",
      "[30]\ttraining's binary_logloss: 0.0669502\tvalid_1's binary_logloss: 0.0454529\n",
      "[35]\ttraining's binary_logloss: 0.0666739\tvalid_1's binary_logloss: 0.0452662\n",
      "[40]\ttraining's binary_logloss: 0.0665038\tvalid_1's binary_logloss: 0.045154\n",
      "[45]\ttraining's binary_logloss: 0.0663908\tvalid_1's binary_logloss: 0.0450846\n",
      "[50]\ttraining's binary_logloss: 0.0663113\tvalid_1's binary_logloss: 0.0450396\n",
      "[55]\ttraining's binary_logloss: 0.0662513\tvalid_1's binary_logloss: 0.045009\n",
      "[60]\ttraining's binary_logloss: 0.0662034\tvalid_1's binary_logloss: 0.0449896\n",
      "[65]\ttraining's binary_logloss: 0.0661581\tvalid_1's binary_logloss: 0.0449722\n",
      "[70]\ttraining's binary_logloss: 0.0661237\tvalid_1's binary_logloss: 0.0449604\n",
      "[75]\ttraining's binary_logloss: 0.0660856\tvalid_1's binary_logloss: 0.04495\n",
      "[80]\ttraining's binary_logloss: 0.0660538\tvalid_1's binary_logloss: 0.0449441\n",
      "[85]\ttraining's binary_logloss: 0.0660219\tvalid_1's binary_logloss: 0.0449373\n",
      "[90]\ttraining's binary_logloss: 0.0659856\tvalid_1's binary_logloss: 0.044929\n",
      "[95]\ttraining's binary_logloss: 0.0659573\tvalid_1's binary_logloss: 0.0449262\n",
      "[100]\ttraining's binary_logloss: 0.0659293\tvalid_1's binary_logloss: 0.0449222\n",
      "[105]\ttraining's binary_logloss: 0.0659023\tvalid_1's binary_logloss: 0.0449196\n",
      "[110]\ttraining's binary_logloss: 0.0658692\tvalid_1's binary_logloss: 0.0449155\n",
      "[115]\ttraining's binary_logloss: 0.065844\tvalid_1's binary_logloss: 0.0449124\n",
      "[120]\ttraining's binary_logloss: 0.0658153\tvalid_1's binary_logloss: 0.0449089\n",
      "[125]\ttraining's binary_logloss: 0.0657881\tvalid_1's binary_logloss: 0.0449061\n",
      "[130]\ttraining's binary_logloss: 0.0657659\tvalid_1's binary_logloss: 0.0449035\n",
      "[135]\ttraining's binary_logloss: 0.0657366\tvalid_1's binary_logloss: 0.0449002\n",
      "[140]\ttraining's binary_logloss: 0.0657081\tvalid_1's binary_logloss: 0.044897\n",
      "[145]\ttraining's binary_logloss: 0.0656801\tvalid_1's binary_logloss: 0.0448931\n",
      "[150]\ttraining's binary_logloss: 0.0656528\tvalid_1's binary_logloss: 0.0448896\n",
      "[155]\ttraining's binary_logloss: 0.0656294\tvalid_1's binary_logloss: 0.0448885\n",
      "[160]\ttraining's binary_logloss: 0.0656061\tvalid_1's binary_logloss: 0.0448871\n",
      "[165]\ttraining's binary_logloss: 0.0655841\tvalid_1's binary_logloss: 0.0448855\n",
      "[170]\ttraining's binary_logloss: 0.0655591\tvalid_1's binary_logloss: 0.0448837\n",
      "[175]\ttraining's binary_logloss: 0.065538\tvalid_1's binary_logloss: 0.0448819\n",
      "[180]\ttraining's binary_logloss: 0.065513\tvalid_1's binary_logloss: 0.0448805\n",
      "[185]\ttraining's binary_logloss: 0.0654891\tvalid_1's binary_logloss: 0.0448792\n",
      "[190]\ttraining's binary_logloss: 0.0654601\tvalid_1's binary_logloss: 0.0448742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195]\ttraining's binary_logloss: 0.0654366\tvalid_1's binary_logloss: 0.0448724\n",
      "[200]\ttraining's binary_logloss: 0.0654181\tvalid_1's binary_logloss: 0.0448714\n",
      "[205]\ttraining's binary_logloss: 0.0653947\tvalid_1's binary_logloss: 0.0448715\n",
      "[210]\ttraining's binary_logloss: 0.0653706\tvalid_1's binary_logloss: 0.0448706\n",
      "[215]\ttraining's binary_logloss: 0.0653445\tvalid_1's binary_logloss: 0.0448689\n",
      "[220]\ttraining's binary_logloss: 0.0653216\tvalid_1's binary_logloss: 0.044867\n",
      "[225]\ttraining's binary_logloss: 0.0652959\tvalid_1's binary_logloss: 0.0448645\n",
      "[230]\ttraining's binary_logloss: 0.0652757\tvalid_1's binary_logloss: 0.0448617\n",
      "[235]\ttraining's binary_logloss: 0.0652513\tvalid_1's binary_logloss: 0.04486\n",
      "[240]\ttraining's binary_logloss: 0.0652266\tvalid_1's binary_logloss: 0.0448591\n",
      "[245]\ttraining's binary_logloss: 0.0652062\tvalid_1's binary_logloss: 0.0448583\n",
      "[250]\ttraining's binary_logloss: 0.0651853\tvalid_1's binary_logloss: 0.0448563\n",
      "[255]\ttraining's binary_logloss: 0.065163\tvalid_1's binary_logloss: 0.0448575\n",
      "[260]\ttraining's binary_logloss: 0.0651419\tvalid_1's binary_logloss: 0.044855\n",
      "[265]\ttraining's binary_logloss: 0.0651219\tvalid_1's binary_logloss: 0.0448545\n",
      "[270]\ttraining's binary_logloss: 0.0650969\tvalid_1's binary_logloss: 0.044853\n",
      "[275]\ttraining's binary_logloss: 0.0650741\tvalid_1's binary_logloss: 0.0448501\n",
      "[280]\ttraining's binary_logloss: 0.0650517\tvalid_1's binary_logloss: 0.0448482\n",
      "[285]\ttraining's binary_logloss: 0.0650297\tvalid_1's binary_logloss: 0.044846\n",
      "[290]\ttraining's binary_logloss: 0.0650131\tvalid_1's binary_logloss: 0.0448449\n",
      "[295]\ttraining's binary_logloss: 0.0649945\tvalid_1's binary_logloss: 0.0448446\n",
      "[300]\ttraining's binary_logloss: 0.0649717\tvalid_1's binary_logloss: 0.0448434\n",
      "[305]\ttraining's binary_logloss: 0.0649525\tvalid_1's binary_logloss: 0.0448429\n",
      "[310]\ttraining's binary_logloss: 0.0649308\tvalid_1's binary_logloss: 0.0448423\n",
      "[315]\ttraining's binary_logloss: 0.064909\tvalid_1's binary_logloss: 0.0448423\n",
      "[320]\ttraining's binary_logloss: 0.0648893\tvalid_1's binary_logloss: 0.0448409\n",
      "[325]\ttraining's binary_logloss: 0.0648692\tvalid_1's binary_logloss: 0.0448403\n",
      "[330]\ttraining's binary_logloss: 0.0648486\tvalid_1's binary_logloss: 0.0448402\n",
      "[335]\ttraining's binary_logloss: 0.0648262\tvalid_1's binary_logloss: 0.0448388\n",
      "[340]\ttraining's binary_logloss: 0.0648081\tvalid_1's binary_logloss: 0.044838\n",
      "[345]\ttraining's binary_logloss: 0.0647893\tvalid_1's binary_logloss: 0.0448386\n",
      "[350]\ttraining's binary_logloss: 0.0647694\tvalid_1's binary_logloss: 0.0448382\n",
      "Early stopping, best iteration is:\n",
      "[340]\ttraining's binary_logloss: 0.0648081\tvalid_1's binary_logloss: 0.044838\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2...\n",
      "before mean: 0.013413005718231752\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "[LightGBM] [Info] Number of positive: 810891, number of negative: 31624749\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.520082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9186\n",
      "[LightGBM] [Info] Number of data points in the train set: 32435640, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025000 -> initscore=-3.663562\n",
      "[LightGBM] [Info] Start training from score -3.663562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[5]\ttraining's binary_logloss: 0.0786394\tvalid_1's binary_logloss: 0.053038\n",
      "[10]\ttraining's binary_logloss: 0.072662\tvalid_1's binary_logloss: 0.0491149\n",
      "[15]\ttraining's binary_logloss: 0.0697931\tvalid_1's binary_logloss: 0.0471762\n",
      "[20]\ttraining's binary_logloss: 0.0682953\tvalid_1's binary_logloss: 0.046147\n",
      "[25]\ttraining's binary_logloss: 0.0674731\tvalid_1's binary_logloss: 0.0455744\n",
      "[30]\ttraining's binary_logloss: 0.067008\tvalid_1's binary_logloss: 0.0452483\n",
      "[35]\ttraining's binary_logloss: 0.0667327\tvalid_1's binary_logloss: 0.0450586\n",
      "[40]\ttraining's binary_logloss: 0.0665632\tvalid_1's binary_logloss: 0.0449466\n",
      "[45]\ttraining's binary_logloss: 0.0664551\tvalid_1's binary_logloss: 0.0448793\n",
      "[50]\ttraining's binary_logloss: 0.0663781\tvalid_1's binary_logloss: 0.0448353\n",
      "[55]\ttraining's binary_logloss: 0.0663209\tvalid_1's binary_logloss: 0.0448085\n",
      "[60]\ttraining's binary_logloss: 0.0662686\tvalid_1's binary_logloss: 0.0447869\n",
      "[65]\ttraining's binary_logloss: 0.0662252\tvalid_1's binary_logloss: 0.0447717\n",
      "[70]\ttraining's binary_logloss: 0.0661818\tvalid_1's binary_logloss: 0.0447571\n",
      "[75]\ttraining's binary_logloss: 0.0661439\tvalid_1's binary_logloss: 0.0447454\n",
      "[80]\ttraining's binary_logloss: 0.0661094\tvalid_1's binary_logloss: 0.0447372\n",
      "[85]\ttraining's binary_logloss: 0.0660752\tvalid_1's binary_logloss: 0.0447315\n",
      "[90]\ttraining's binary_logloss: 0.0660417\tvalid_1's binary_logloss: 0.0447257\n",
      "[95]\ttraining's binary_logloss: 0.066011\tvalid_1's binary_logloss: 0.0447213\n",
      "[100]\ttraining's binary_logloss: 0.0659773\tvalid_1's binary_logloss: 0.044715\n",
      "[105]\ttraining's binary_logloss: 0.0659489\tvalid_1's binary_logloss: 0.0447097\n",
      "[110]\ttraining's binary_logloss: 0.0659223\tvalid_1's binary_logloss: 0.0447063\n",
      "[115]\ttraining's binary_logloss: 0.0658955\tvalid_1's binary_logloss: 0.0447028\n",
      "[120]\ttraining's binary_logloss: 0.0658693\tvalid_1's binary_logloss: 0.044699\n",
      "[125]\ttraining's binary_logloss: 0.0658457\tvalid_1's binary_logloss: 0.0446969\n",
      "[130]\ttraining's binary_logloss: 0.0658164\tvalid_1's binary_logloss: 0.0446935\n",
      "[135]\ttraining's binary_logloss: 0.0657916\tvalid_1's binary_logloss: 0.0446921\n",
      "[140]\ttraining's binary_logloss: 0.065764\tvalid_1's binary_logloss: 0.0446885\n",
      "[145]\ttraining's binary_logloss: 0.0657379\tvalid_1's binary_logloss: 0.0446859\n",
      "[150]\ttraining's binary_logloss: 0.0657144\tvalid_1's binary_logloss: 0.0446839\n",
      "[155]\ttraining's binary_logloss: 0.0656909\tvalid_1's binary_logloss: 0.0446803\n",
      "[160]\ttraining's binary_logloss: 0.0656686\tvalid_1's binary_logloss: 0.0446789\n",
      "[165]\ttraining's binary_logloss: 0.065644\tvalid_1's binary_logloss: 0.0446758\n",
      "[170]\ttraining's binary_logloss: 0.0656192\tvalid_1's binary_logloss: 0.0446744\n",
      "[175]\ttraining's binary_logloss: 0.0655957\tvalid_1's binary_logloss: 0.0446709\n",
      "[180]\ttraining's binary_logloss: 0.0655701\tvalid_1's binary_logloss: 0.0446707\n",
      "[185]\ttraining's binary_logloss: 0.0655447\tvalid_1's binary_logloss: 0.0446694\n",
      "[190]\ttraining's binary_logloss: 0.065521\tvalid_1's binary_logloss: 0.0446679\n",
      "[195]\ttraining's binary_logloss: 0.0654969\tvalid_1's binary_logloss: 0.0446657\n",
      "[200]\ttraining's binary_logloss: 0.065477\tvalid_1's binary_logloss: 0.0446628\n",
      "[205]\ttraining's binary_logloss: 0.0654566\tvalid_1's binary_logloss: 0.044662\n",
      "[210]\ttraining's binary_logloss: 0.0654329\tvalid_1's binary_logloss: 0.0446613\n",
      "[215]\ttraining's binary_logloss: 0.0654112\tvalid_1's binary_logloss: 0.0446605\n",
      "[220]\ttraining's binary_logloss: 0.0653896\tvalid_1's binary_logloss: 0.044659\n",
      "[225]\ttraining's binary_logloss: 0.0653697\tvalid_1's binary_logloss: 0.0446566\n",
      "[230]\ttraining's binary_logloss: 0.0653461\tvalid_1's binary_logloss: 0.0446551\n",
      "[235]\ttraining's binary_logloss: 0.0653225\tvalid_1's binary_logloss: 0.0446541\n",
      "[240]\ttraining's binary_logloss: 0.0653008\tvalid_1's binary_logloss: 0.0446527\n",
      "[245]\ttraining's binary_logloss: 0.065277\tvalid_1's binary_logloss: 0.0446501\n",
      "[250]\ttraining's binary_logloss: 0.0652542\tvalid_1's binary_logloss: 0.0446474\n",
      "[255]\ttraining's binary_logloss: 0.0652357\tvalid_1's binary_logloss: 0.0446474\n",
      "[260]\ttraining's binary_logloss: 0.0652166\tvalid_1's binary_logloss: 0.0446468\n",
      "[265]\ttraining's binary_logloss: 0.0651934\tvalid_1's binary_logloss: 0.0446448\n",
      "[270]\ttraining's binary_logloss: 0.0651739\tvalid_1's binary_logloss: 0.0446438\n",
      "[275]\ttraining's binary_logloss: 0.0651526\tvalid_1's binary_logloss: 0.044643\n",
      "[280]\ttraining's binary_logloss: 0.0651305\tvalid_1's binary_logloss: 0.0446407\n",
      "[285]\ttraining's binary_logloss: 0.065106\tvalid_1's binary_logloss: 0.0446397\n",
      "[290]\ttraining's binary_logloss: 0.0650854\tvalid_1's binary_logloss: 0.044639\n",
      "[295]\ttraining's binary_logloss: 0.0650653\tvalid_1's binary_logloss: 0.0446372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttraining's binary_logloss: 0.0650479\tvalid_1's binary_logloss: 0.0446373\n",
      "[305]\ttraining's binary_logloss: 0.0650244\tvalid_1's binary_logloss: 0.0446364\n",
      "[310]\ttraining's binary_logloss: 0.0650083\tvalid_1's binary_logloss: 0.0446354\n",
      "[315]\ttraining's binary_logloss: 0.0649871\tvalid_1's binary_logloss: 0.0446337\n",
      "[320]\ttraining's binary_logloss: 0.0649646\tvalid_1's binary_logloss: 0.0446313\n",
      "[325]\ttraining's binary_logloss: 0.064944\tvalid_1's binary_logloss: 0.0446306\n",
      "[330]\ttraining's binary_logloss: 0.064923\tvalid_1's binary_logloss: 0.0446287\n",
      "[335]\ttraining's binary_logloss: 0.0649016\tvalid_1's binary_logloss: 0.0446276\n",
      "[340]\ttraining's binary_logloss: 0.0648796\tvalid_1's binary_logloss: 0.0446281\n",
      "Early stopping, best iteration is:\n",
      "[334]\ttraining's binary_logloss: 0.0649056\tvalid_1's binary_logloss: 0.0446273\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3...\n",
      "before mean: 0.013395174443542239\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "[LightGBM] [Info] Number of positive: 809813, number of negative: 31582707\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.292205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9192\n",
      "[LightGBM] [Info] Number of data points in the train set: 32392520, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025000 -> initscore=-3.663562\n",
      "[LightGBM] [Info] Start training from score -3.663562\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[5]\ttraining's binary_logloss: 0.0786244\tvalid_1's binary_logloss: 0.0532009\n",
      "[10]\ttraining's binary_logloss: 0.0726434\tvalid_1's binary_logloss: 0.0492771\n",
      "[15]\ttraining's binary_logloss: 0.0697758\tvalid_1's binary_logloss: 0.0473406\n",
      "[20]\ttraining's binary_logloss: 0.0682771\tvalid_1's binary_logloss: 0.0463109\n",
      "[25]\ttraining's binary_logloss: 0.0674577\tvalid_1's binary_logloss: 0.0457416\n",
      "[30]\ttraining's binary_logloss: 0.0669906\tvalid_1's binary_logloss: 0.0454147\n",
      "[35]\ttraining's binary_logloss: 0.0667122\tvalid_1's binary_logloss: 0.0452234\n",
      "[40]\ttraining's binary_logloss: 0.0665454\tvalid_1's binary_logloss: 0.0451119\n",
      "[45]\ttraining's binary_logloss: 0.0664356\tvalid_1's binary_logloss: 0.0450426\n",
      "[50]\ttraining's binary_logloss: 0.0663563\tvalid_1's binary_logloss: 0.0449983\n",
      "[55]\ttraining's binary_logloss: 0.066296\tvalid_1's binary_logloss: 0.0449675\n",
      "[60]\ttraining's binary_logloss: 0.0662463\tvalid_1's binary_logloss: 0.0449463\n",
      "[65]\ttraining's binary_logloss: 0.0662021\tvalid_1's binary_logloss: 0.04493\n",
      "[70]\ttraining's binary_logloss: 0.0661626\tvalid_1's binary_logloss: 0.0449182\n",
      "[75]\ttraining's binary_logloss: 0.0661261\tvalid_1's binary_logloss: 0.0449084\n",
      "[80]\ttraining's binary_logloss: 0.0660888\tvalid_1's binary_logloss: 0.0449001\n",
      "[85]\ttraining's binary_logloss: 0.066055\tvalid_1's binary_logloss: 0.0448944\n",
      "[90]\ttraining's binary_logloss: 0.0660279\tvalid_1's binary_logloss: 0.0448927\n",
      "[95]\ttraining's binary_logloss: 0.0659938\tvalid_1's binary_logloss: 0.0448852\n",
      "[100]\ttraining's binary_logloss: 0.0659648\tvalid_1's binary_logloss: 0.0448824\n",
      "[105]\ttraining's binary_logloss: 0.0659361\tvalid_1's binary_logloss: 0.0448765\n",
      "[110]\ttraining's binary_logloss: 0.0659088\tvalid_1's binary_logloss: 0.0448733\n",
      "[115]\ttraining's binary_logloss: 0.0658804\tvalid_1's binary_logloss: 0.0448694\n",
      "[120]\ttraining's binary_logloss: 0.0658494\tvalid_1's binary_logloss: 0.044865\n",
      "[125]\ttraining's binary_logloss: 0.0658211\tvalid_1's binary_logloss: 0.0448617\n",
      "[130]\ttraining's binary_logloss: 0.0657915\tvalid_1's binary_logloss: 0.0448594\n",
      "[135]\ttraining's binary_logloss: 0.0657657\tvalid_1's binary_logloss: 0.0448562\n",
      "[140]\ttraining's binary_logloss: 0.0657377\tvalid_1's binary_logloss: 0.0448529\n",
      "[145]\ttraining's binary_logloss: 0.0657109\tvalid_1's binary_logloss: 0.0448505\n",
      "[150]\ttraining's binary_logloss: 0.0656887\tvalid_1's binary_logloss: 0.0448483\n",
      "[155]\ttraining's binary_logloss: 0.065663\tvalid_1's binary_logloss: 0.0448452\n",
      "[160]\ttraining's binary_logloss: 0.0656399\tvalid_1's binary_logloss: 0.0448442\n",
      "[165]\ttraining's binary_logloss: 0.0656163\tvalid_1's binary_logloss: 0.0448409\n",
      "[170]\ttraining's binary_logloss: 0.0655944\tvalid_1's binary_logloss: 0.0448407\n",
      "[175]\ttraining's binary_logloss: 0.0655687\tvalid_1's binary_logloss: 0.0448387\n",
      "[180]\ttraining's binary_logloss: 0.0655448\tvalid_1's binary_logloss: 0.0448388\n",
      "[185]\ttraining's binary_logloss: 0.0655163\tvalid_1's binary_logloss: 0.0448362\n",
      "[190]\ttraining's binary_logloss: 0.0654914\tvalid_1's binary_logloss: 0.0448328\n",
      "[195]\ttraining's binary_logloss: 0.065468\tvalid_1's binary_logloss: 0.0448299\n",
      "[200]\ttraining's binary_logloss: 0.0654432\tvalid_1's binary_logloss: 0.0448266\n",
      "[205]\ttraining's binary_logloss: 0.0654188\tvalid_1's binary_logloss: 0.044823\n",
      "[210]\ttraining's binary_logloss: 0.0653962\tvalid_1's binary_logloss: 0.0448231\n",
      "[215]\ttraining's binary_logloss: 0.0653748\tvalid_1's binary_logloss: 0.0448223\n",
      "[220]\ttraining's binary_logloss: 0.0653544\tvalid_1's binary_logloss: 0.0448218\n",
      "[225]\ttraining's binary_logloss: 0.0653316\tvalid_1's binary_logloss: 0.0448194\n",
      "[230]\ttraining's binary_logloss: 0.0653071\tvalid_1's binary_logloss: 0.044818\n",
      "[235]\ttraining's binary_logloss: 0.0652836\tvalid_1's binary_logloss: 0.0448178\n",
      "[240]\ttraining's binary_logloss: 0.0652647\tvalid_1's binary_logloss: 0.0448164\n",
      "[245]\ttraining's binary_logloss: 0.0652425\tvalid_1's binary_logloss: 0.0448166\n",
      "[250]\ttraining's binary_logloss: 0.0652194\tvalid_1's binary_logloss: 0.0448134\n",
      "[255]\ttraining's binary_logloss: 0.0651962\tvalid_1's binary_logloss: 0.0448129\n",
      "[260]\ttraining's binary_logloss: 0.0651758\tvalid_1's binary_logloss: 0.0448127\n",
      "[265]\ttraining's binary_logloss: 0.0651536\tvalid_1's binary_logloss: 0.0448116\n",
      "[270]\ttraining's binary_logloss: 0.0651319\tvalid_1's binary_logloss: 0.0448112\n",
      "[275]\ttraining's binary_logloss: 0.0651087\tvalid_1's binary_logloss: 0.0448098\n",
      "[280]\ttraining's binary_logloss: 0.0650913\tvalid_1's binary_logloss: 0.0448086\n",
      "[285]\ttraining's binary_logloss: 0.0650696\tvalid_1's binary_logloss: 0.0448078\n",
      "[290]\ttraining's binary_logloss: 0.0650513\tvalid_1's binary_logloss: 0.0448066\n",
      "[295]\ttraining's binary_logloss: 0.06503\tvalid_1's binary_logloss: 0.0448061\n",
      "[300]\ttraining's binary_logloss: 0.0650093\tvalid_1's binary_logloss: 0.044806\n",
      "[305]\ttraining's binary_logloss: 0.0649892\tvalid_1's binary_logloss: 0.0448046\n",
      "[310]\ttraining's binary_logloss: 0.0649661\tvalid_1's binary_logloss: 0.0448036\n",
      "[315]\ttraining's binary_logloss: 0.0649442\tvalid_1's binary_logloss: 0.0448029\n",
      "[320]\ttraining's binary_logloss: 0.0649258\tvalid_1's binary_logloss: 0.0448019\n",
      "[325]\ttraining's binary_logloss: 0.0649045\tvalid_1's binary_logloss: 0.0448017\n",
      "[330]\ttraining's binary_logloss: 0.064883\tvalid_1's binary_logloss: 0.0448012\n",
      "[335]\ttraining's binary_logloss: 0.0648616\tvalid_1's binary_logloss: 0.0447995\n",
      "[340]\ttraining's binary_logloss: 0.0648413\tvalid_1's binary_logloss: 0.0447999\n",
      "[345]\ttraining's binary_logloss: 0.0648242\tvalid_1's binary_logloss: 0.0448009\n",
      "Early stopping, best iteration is:\n",
      "[336]\ttraining's binary_logloss: 0.0648567\tvalid_1's binary_logloss: 0.044799\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4...\n",
      "before mean: 0.013392544413230891\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "[LightGBM] [Info] Number of positive: 809654, number of negative: 31576506\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.653833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9192\n",
      "[LightGBM] [Info] Number of data points in the train set: 32386160, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025000 -> initscore=-3.663562\n",
      "[LightGBM] [Info] Start training from score -3.663562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[5]\ttraining's binary_logloss: 0.0785848\tvalid_1's binary_logloss: 0.0533342\n",
      "[10]\ttraining's binary_logloss: 0.0726019\tvalid_1's binary_logloss: 0.0494165\n",
      "[15]\ttraining's binary_logloss: 0.0697295\tvalid_1's binary_logloss: 0.0474823\n",
      "[20]\ttraining's binary_logloss: 0.0682307\tvalid_1's binary_logloss: 0.0464571\n",
      "[25]\ttraining's binary_logloss: 0.0674094\tvalid_1's binary_logloss: 0.0458917\n",
      "[30]\ttraining's binary_logloss: 0.0669437\tvalid_1's binary_logloss: 0.0455702\n",
      "[35]\ttraining's binary_logloss: 0.0666677\tvalid_1's binary_logloss: 0.0453829\n",
      "[40]\ttraining's binary_logloss: 0.0664965\tvalid_1's binary_logloss: 0.0452701\n",
      "[45]\ttraining's binary_logloss: 0.0663859\tvalid_1's binary_logloss: 0.0452046\n",
      "[50]\ttraining's binary_logloss: 0.0663068\tvalid_1's binary_logloss: 0.0451586\n",
      "[55]\ttraining's binary_logloss: 0.0662452\tvalid_1's binary_logloss: 0.0451277\n",
      "[60]\ttraining's binary_logloss: 0.0661937\tvalid_1's binary_logloss: 0.0451056\n",
      "[65]\ttraining's binary_logloss: 0.0661493\tvalid_1's binary_logloss: 0.0450907\n",
      "[70]\ttraining's binary_logloss: 0.0661071\tvalid_1's binary_logloss: 0.0450764\n",
      "[75]\ttraining's binary_logloss: 0.0660691\tvalid_1's binary_logloss: 0.0450653\n",
      "[80]\ttraining's binary_logloss: 0.0660359\tvalid_1's binary_logloss: 0.0450593\n",
      "[85]\ttraining's binary_logloss: 0.0660006\tvalid_1's binary_logloss: 0.0450505\n",
      "[90]\ttraining's binary_logloss: 0.0659675\tvalid_1's binary_logloss: 0.045045\n",
      "[95]\ttraining's binary_logloss: 0.0659382\tvalid_1's binary_logloss: 0.0450401\n",
      "[100]\ttraining's binary_logloss: 0.0659076\tvalid_1's binary_logloss: 0.0450362\n",
      "[105]\ttraining's binary_logloss: 0.0658831\tvalid_1's binary_logloss: 0.0450346\n",
      "[110]\ttraining's binary_logloss: 0.0658546\tvalid_1's binary_logloss: 0.0450313\n",
      "[115]\ttraining's binary_logloss: 0.0658256\tvalid_1's binary_logloss: 0.0450286\n",
      "[120]\ttraining's binary_logloss: 0.0657993\tvalid_1's binary_logloss: 0.0450261\n",
      "[125]\ttraining's binary_logloss: 0.0657722\tvalid_1's binary_logloss: 0.0450237\n",
      "[130]\ttraining's binary_logloss: 0.0657459\tvalid_1's binary_logloss: 0.045021\n",
      "[135]\ttraining's binary_logloss: 0.0657163\tvalid_1's binary_logloss: 0.0450153\n",
      "[140]\ttraining's binary_logloss: 0.0656926\tvalid_1's binary_logloss: 0.0450141\n",
      "[145]\ttraining's binary_logloss: 0.0656694\tvalid_1's binary_logloss: 0.0450124\n",
      "[150]\ttraining's binary_logloss: 0.0656432\tvalid_1's binary_logloss: 0.0450094\n",
      "[155]\ttraining's binary_logloss: 0.0656201\tvalid_1's binary_logloss: 0.0450069\n",
      "[160]\ttraining's binary_logloss: 0.065595\tvalid_1's binary_logloss: 0.0450043\n",
      "[165]\ttraining's binary_logloss: 0.0655676\tvalid_1's binary_logloss: 0.045001\n",
      "[170]\ttraining's binary_logloss: 0.0655417\tvalid_1's binary_logloss: 0.0449986\n",
      "[175]\ttraining's binary_logloss: 0.0655149\tvalid_1's binary_logloss: 0.0449957\n",
      "[180]\ttraining's binary_logloss: 0.0654919\tvalid_1's binary_logloss: 0.0449944\n",
      "[185]\ttraining's binary_logloss: 0.0654668\tvalid_1's binary_logloss: 0.0449927\n",
      "[190]\ttraining's binary_logloss: 0.0654454\tvalid_1's binary_logloss: 0.0449915\n",
      "[195]\ttraining's binary_logloss: 0.0654199\tvalid_1's binary_logloss: 0.0449892\n",
      "[200]\ttraining's binary_logloss: 0.0653972\tvalid_1's binary_logloss: 0.0449861\n",
      "[205]\ttraining's binary_logloss: 0.0653723\tvalid_1's binary_logloss: 0.0449849\n",
      "[210]\ttraining's binary_logloss: 0.065349\tvalid_1's binary_logloss: 0.044984\n",
      "[215]\ttraining's binary_logloss: 0.0653249\tvalid_1's binary_logloss: 0.0449823\n",
      "[220]\ttraining's binary_logloss: 0.0653046\tvalid_1's binary_logloss: 0.0449794\n",
      "[225]\ttraining's binary_logloss: 0.0652796\tvalid_1's binary_logloss: 0.0449771\n",
      "[230]\ttraining's binary_logloss: 0.0652568\tvalid_1's binary_logloss: 0.0449749\n",
      "[235]\ttraining's binary_logloss: 0.0652354\tvalid_1's binary_logloss: 0.044975\n",
      "[240]\ttraining's binary_logloss: 0.0652155\tvalid_1's binary_logloss: 0.0449741\n",
      "[245]\ttraining's binary_logloss: 0.065193\tvalid_1's binary_logloss: 0.0449731\n",
      "[250]\ttraining's binary_logloss: 0.0651737\tvalid_1's binary_logloss: 0.0449714\n",
      "[255]\ttraining's binary_logloss: 0.0651499\tvalid_1's binary_logloss: 0.044971\n",
      "[260]\ttraining's binary_logloss: 0.0651284\tvalid_1's binary_logloss: 0.0449692\n",
      "[265]\ttraining's binary_logloss: 0.0651096\tvalid_1's binary_logloss: 0.0449686\n",
      "[270]\ttraining's binary_logloss: 0.0650889\tvalid_1's binary_logloss: 0.0449684\n",
      "[275]\ttraining's binary_logloss: 0.065067\tvalid_1's binary_logloss: 0.0449664\n",
      "[280]\ttraining's binary_logloss: 0.0650496\tvalid_1's binary_logloss: 0.0449648\n",
      "[285]\ttraining's binary_logloss: 0.0650268\tvalid_1's binary_logloss: 0.0449645\n",
      "[290]\ttraining's binary_logloss: 0.0650061\tvalid_1's binary_logloss: 0.0449624\n",
      "[295]\ttraining's binary_logloss: 0.064987\tvalid_1's binary_logloss: 0.0449622\n",
      "[300]\ttraining's binary_logloss: 0.0649714\tvalid_1's binary_logloss: 0.0449626\n",
      "[305]\ttraining's binary_logloss: 0.0649478\tvalid_1's binary_logloss: 0.044961\n",
      "[310]\ttraining's binary_logloss: 0.0649252\tvalid_1's binary_logloss: 0.044961\n",
      "[315]\ttraining's binary_logloss: 0.0649027\tvalid_1's binary_logloss: 0.0449601\n",
      "[320]\ttraining's binary_logloss: 0.0648824\tvalid_1's binary_logloss: 0.0449599\n",
      "[325]\ttraining's binary_logloss: 0.0648609\tvalid_1's binary_logloss: 0.0449584\n",
      "[330]\ttraining's binary_logloss: 0.0648413\tvalid_1's binary_logloss: 0.0449576\n",
      "[335]\ttraining's binary_logloss: 0.0648226\tvalid_1's binary_logloss: 0.0449564\n",
      "[340]\ttraining's binary_logloss: 0.0648041\tvalid_1's binary_logloss: 0.0449569\n",
      "Early stopping, best iteration is:\n",
      "[334]\ttraining's binary_logloss: 0.0648273\tvalid_1's binary_logloss: 0.0449563\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}...')\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        #num_boost_round = 10500,\n",
    "        num_boost_round = 1000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 10,\n",
    "        verbose_eval = 5,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 295362 1544564 217742 1694360 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098534_clicks</td>\n",
       "      <td>223062 908024 530377 1342293 1607945 1649004 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098535_clicks</td>\n",
       "      <td>745365 1750442 803918 236461 1320151 226839 89...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098538_clicks</td>\n",
       "      <td>1263747 52785 1570378 1550143 703265 351587 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098539_clicks</td>\n",
       "      <td>631008 1408458 1658802 1057728 1251433 617897 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012730</th>\n",
       "      <td>12899773_clicks</td>\n",
       "      <td>1311526 1484665 1578804 184006 337571 946627 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012731</th>\n",
       "      <td>12899774_clicks</td>\n",
       "      <td>33035 1539309 771913 819288 270852 218795 9548...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012732</th>\n",
       "      <td>12899775_clicks</td>\n",
       "      <td>1743151 1760714 1163166 1255910 1022572 832192...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012733</th>\n",
       "      <td>12899777_clicks</td>\n",
       "      <td>384045 1308634 1688215 703474 395762 364190 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012734</th>\n",
       "      <td>12899778_clicks</td>\n",
       "      <td>561560 1167224 13942 566042 570506 32070 18271...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1012735 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098529_clicks  1105029 459126 295362 1544564 217742 1694360 1...\n",
       "1        11098534_clicks  223062 908024 530377 1342293 1607945 1649004 1...\n",
       "2        11098535_clicks  745365 1750442 803918 236461 1320151 226839 89...\n",
       "3        11098538_clicks  1263747 52785 1570378 1550143 703265 351587 17...\n",
       "4        11098539_clicks  631008 1408458 1658802 1057728 1251433 617897 ...\n",
       "...                  ...                                                ...\n",
       "1012730  12899773_clicks  1311526 1484665 1578804 184006 337571 946627 1...\n",
       "1012731  12899774_clicks  33035 1539309 771913 819288 270852 218795 9548...\n",
       "1012732  12899775_clicks  1743151 1760714 1163166 1255910 1022572 832192...\n",
       "1012733  12899777_clicks  384045 1308634 1688215 703474 395762 364190 14...\n",
       "1012734  12899778_clicks  561560 1167224 13942 566042 570506 32070 18271...\n",
       "\n",
       "[1012735 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5343416874865425\n"
     ]
    }
   ],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# clicks recall = 0.5271239406357268 おためしtop20, , PB = 0.579\n",
    "\n",
    "# baseline top20のitem2itemを使ってgenerateしたもの, trainsform, duplicate削減、negativeのみremove\n",
    "# clicks recall = 0.5279590141803007 num=100 きた！\n",
    "# 既存データ + 50までbackfill, 20位まで num=100 clicks recall = 0.5289963053976738 きた！\n",
    "#                                               orders recall = 0.6531281219777659\n",
    "# 既存データ + 50までbackfill, 30位まで num=100 orders recall = 0.6533100544839979\n",
    "# 既存データ + 50までbackfill, 50位まで num=100 orders recall = 0.6536483851096223\n",
    "# 既存データ + 50までbackfill, 50位まで num=1000(137) orders recall = 0.6536451933112674\n",
    "\n",
    "# under samplingなしだと上位50で2.1%がpositive\n",
    "# 既存データ + 50までbackfill under sampling pos:neg = 1:2 33% pos, orders recall = 0.6190460991436405\n",
    "#                                            pos:neg = 1:9 10% pos, orders recall = 0.6536036999326531\n",
    "#                                            pos:neg = 1:19 5% pos, orders recall = 0.6536388097145575 ちょい下がるけどそんなに問題なさそう\n",
    "#                                            pos:neg = 1:39 2.5% pos,orders recall= 0.6536930702865916 これくらいの比率で固定しよう, PB = 0.580\n",
    "#                                                                    carts recall = 0.41731398378440265\n",
    "#                                                                    clicks recall = 0.5295727681719636\n",
    "# feature増版、click i2i, top10,20 pos:neg = 1:39 2.5%, num=100 orders recall = 0.6538813863895334\n",
    "#                                                      num=1000 orders recall = 0.654031400912216 , PB = 0.581\n",
    "#                                                      num=1000 carts recall = 0.41827325050912256 \n",
    "#                                                      num=1000 clicks recall = 0.5309427217017728\n",
    "# aid feature追加 2weeks, 4weeks                                orders recall = 0.6575391873043028 ほぼ変わらんのでこっち\n",
    "# 2,3,4 weeks                                                   orders recall = 0.6575551462960776\n",
    "#                                                               \n",
    "\t\t\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NS3YOf-UgmrY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCS32wgTySui6Eefy8UVUd",
   "machine_shape": "hm",
   "mount_file_id": "1iVbo0UFcLudZgX1_ABtKGBC63PFhmRFF",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
