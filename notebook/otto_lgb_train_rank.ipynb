{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7228,
     "status": "ok",
     "timestamp": 1674477402817,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e",
    "outputId": "9e930ba3-d13a-4ee8-f28a-a681a3840d33"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    notebook_path = base_path + '/otto/notebook'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'\n",
    "    notebook_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1674477405015,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "sys.path.append(f\"{notebook_path}/../src/\")\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18480,
     "status": "ok",
     "timestamp": 1674477423492,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_0.parquet') # 0.592\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_0_ver2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "OPTUNA_FLAG = False\n",
    "CROSS_TARGET_STACKING = True\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL_ID = ['session','aid']\n",
    "\n",
    "#TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "TYPE_MODE = 'orders'\n",
    "IGNORE_COL_TARGET = ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    #pos_neg_ratio = 1/39\n",
    "    pos_neg_ratio = 1/29 # 3.3%\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'clicks']\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['orders', 'clicks']\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'clicks']\n",
    "\n",
    "session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df, _target):\n",
    "    true_df = df.groupby('session')[_target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio, random_state=0).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "    # sessionの順番がばらばらになるので再びsort\n",
    "    tmp = pd.concat([after_x, after_y], axis=1).sort_values('session')\n",
    "    after_y = tmp[target]\n",
    "    after_x = tmp.drop(target , axis=1)\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y\n",
    "\n",
    "# dataframe, target_list, number of split data, number of fold\n",
    "def add_cross_stacking_feature(df, cross_target_list, n_div, n_fold, base_path):\n",
    "    # split data in order to save memory\n",
    "    Nrow = df.shape[0]\n",
    "    n = int(Nrow // n_div) + 1\n",
    "    df_list = []\n",
    "    for i in range(n_div):\n",
    "        tmp = df.iloc[i*n : (i+1)*n, :]\n",
    "        df_list.append(tmp)\n",
    "    \n",
    "    # initialization\n",
    "    res_df = pd.DataFrame(columns=[], index = [])\n",
    "    for i, t in enumerate(cross_target_list):\n",
    "        df_pred = np.zeros(Nrow)\n",
    "        for fold in range(n_fold):\n",
    "            print('stacking target:', t, 'fold:', fold)\n",
    "            model = np.load(f'{base_path}/otto/otto_lgbm_fold{fold}_{t}.pkl', allow_pickle=True)\n",
    "            \n",
    "            df_pred_list = []\n",
    "            for i, v in enumerate(df_list):\n",
    "                tmp = model.predict(v)\n",
    "                df_pred_list.append(tmp)\n",
    "            df_pred += np.concatenate(df_pred_list)\n",
    "            \n",
    "        tmp_df = pd.DataFrame(df_pred/n_fold, columns=[f'pred_stack_{t}'], dtype='float64')\n",
    "        if i == 0:\n",
    "            res_df = tmp_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df, tmp_df], axis=1)\n",
    "            \n",
    "    return pd.concat([df, res_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XBANiP7Wfvwt"
   },
   "outputs": [],
   "source": [
    "# importanceが極端に低いものを削る (18件)\n",
    "def remove_features(df):\n",
    "    DROP_COL = ['session_type_mean']\n",
    "    return df.drop(DROP_COL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5102,
     "status": "ok",
     "timestamp": 1674477428581,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = fe.reduce_memory(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10653,
     "status": "ok",
     "timestamp": 1674477439212,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eUrB2zTWf4ZK",
    "outputId": "26a3ff7f-f3fd-4845-d868-18a0146fc11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sum: 224413\n",
      "target mean: 0.009848794650219957\n"
     ]
    }
   ],
   "source": [
    "train = remove_negative_session(train, target)\n",
    "print('target sum:', train[target].sum())\n",
    "print('target mean:', train[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Hyperparameter Tuning by Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    session = train['session']\n",
    "    unique_session = session.unique()\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [20],\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    N_splits = 5\n",
    "    kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}/{N_splits}....')\n",
    "        # session単位で分割してKFoldする\n",
    "        tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "        is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "        del tr_groups, va_groups\n",
    "        gc.collect()\n",
    "        # is_ir, is_va=Trueのindexを取得\n",
    "        trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "        del is_tr, is_va\n",
    "        gc.collect()\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        # under sampling\n",
    "        x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "        # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "        query_list_train = x_train['session'].value_counts()\n",
    "        query_list_train = query_list_train.sort_index()\n",
    "\n",
    "        query_list_valid = x_val['session'].value_counts()\n",
    "        query_list_valid = query_list_valid.sort_index()\n",
    "        \n",
    "\n",
    "        # memory節約のため, under sampling後にfeature追加\n",
    "        print('add session features....')\n",
    "        x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "        print('add aid features....')\n",
    "        x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "        print('add interactive features....')\n",
    "        x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "        print('remove features....')\n",
    "        x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "        print('remove id from features....')\n",
    "        x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 20,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439214,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "params = {'objective': 'lambdarank', \n",
    "          'metric': 'ndcg', \n",
    "          'ndcg_eval_at': [20], \n",
    "          'boosting': 'gbdt', \n",
    "          'seed': 42, \n",
    "          'n_jobs': -1, \n",
    "          #'learning_rate': 0.1, \n",
    "          'learning_rate': 0.05, \n",
    "          'feature_pre_filter': False, \n",
    "          'lambda_l1': 1.7510743847807332e-08, \n",
    "          'lambda_l2': 3.773149139134113e-07, \n",
    "          'num_leaves': 108, \n",
    "          'feature_fraction': 0.4, \n",
    "          'bagging_fraction': 1.0, \n",
    "          'bagging_freq': 0, \n",
    "          'min_child_samples': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "9cc7f1c4-d87f-47b3-c476-5f23190d3cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0/5....\n",
      "before mean: 0.009829630132793749\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7165960, 173)\n",
      "cross_feature stacking, x_train...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "cross_feature stacking, x_val...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.168589 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37409\n",
      "[LightGBM] [Info] Number of data points in the train set: 7165960, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.884676\tvalid_1's ndcg@20: 0.819734\n",
      "[20]\ttraining's ndcg@20: 0.886415\tvalid_1's ndcg@20: 0.821548\n",
      "[30]\ttraining's ndcg@20: 0.88741\tvalid_1's ndcg@20: 0.82242\n",
      "[40]\ttraining's ndcg@20: 0.888415\tvalid_1's ndcg@20: 0.822733\n",
      "[50]\ttraining's ndcg@20: 0.889619\tvalid_1's ndcg@20: 0.823302\n",
      "[60]\ttraining's ndcg@20: 0.890803\tvalid_1's ndcg@20: 0.823712\n",
      "[70]\ttraining's ndcg@20: 0.89194\tvalid_1's ndcg@20: 0.824115\n",
      "[80]\ttraining's ndcg@20: 0.893006\tvalid_1's ndcg@20: 0.824441\n",
      "[90]\ttraining's ndcg@20: 0.894156\tvalid_1's ndcg@20: 0.825014\n",
      "[100]\ttraining's ndcg@20: 0.895475\tvalid_1's ndcg@20: 0.824983\n",
      "[110]\ttraining's ndcg@20: 0.896423\tvalid_1's ndcg@20: 0.825285\n",
      "[120]\ttraining's ndcg@20: 0.897511\tvalid_1's ndcg@20: 0.825457\n",
      "[130]\ttraining's ndcg@20: 0.898565\tvalid_1's ndcg@20: 0.826001\n",
      "[140]\ttraining's ndcg@20: 0.899418\tvalid_1's ndcg@20: 0.825856\n",
      "[150]\ttraining's ndcg@20: 0.90023\tvalid_1's ndcg@20: 0.825816\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttraining's ndcg@20: 0.898666\tvalid_1's ndcg@20: 0.826031\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 0 orders recall = 0.728258230131474\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1/5....\n",
      "before mean: 0.00985302291350443\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7188760, 173)\n",
      "cross_feature stacking, x_train...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "cross_feature stacking, x_val...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.020737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37359\n",
      "[LightGBM] [Info] Number of data points in the train set: 7188760, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.885058\tvalid_1's ndcg@20: 0.819663\n",
      "[20]\ttraining's ndcg@20: 0.887032\tvalid_1's ndcg@20: 0.820992\n",
      "[30]\ttraining's ndcg@20: 0.888054\tvalid_1's ndcg@20: 0.821768\n",
      "[40]\ttraining's ndcg@20: 0.888827\tvalid_1's ndcg@20: 0.822382\n",
      "[50]\ttraining's ndcg@20: 0.889941\tvalid_1's ndcg@20: 0.823119\n",
      "[60]\ttraining's ndcg@20: 0.891065\tvalid_1's ndcg@20: 0.823607\n",
      "[70]\ttraining's ndcg@20: 0.892086\tvalid_1's ndcg@20: 0.823778\n",
      "[80]\ttraining's ndcg@20: 0.893198\tvalid_1's ndcg@20: 0.824421\n",
      "[90]\ttraining's ndcg@20: 0.894368\tvalid_1's ndcg@20: 0.824721\n",
      "[100]\ttraining's ndcg@20: 0.895506\tvalid_1's ndcg@20: 0.825164\n",
      "[110]\ttraining's ndcg@20: 0.896677\tvalid_1's ndcg@20: 0.825598\n",
      "[120]\ttraining's ndcg@20: 0.89761\tvalid_1's ndcg@20: 0.825892\n",
      "[130]\ttraining's ndcg@20: 0.898559\tvalid_1's ndcg@20: 0.826006\n",
      "[140]\ttraining's ndcg@20: 0.899455\tvalid_1's ndcg@20: 0.826079\n",
      "[150]\ttraining's ndcg@20: 0.900336\tvalid_1's ndcg@20: 0.826155\n",
      "[160]\ttraining's ndcg@20: 0.901111\tvalid_1's ndcg@20: 0.826154\n",
      "[170]\ttraining's ndcg@20: 0.90186\tvalid_1's ndcg@20: 0.826119\n",
      "[180]\ttraining's ndcg@20: 0.902708\tvalid_1's ndcg@20: 0.826254\n",
      "[190]\ttraining's ndcg@20: 0.903625\tvalid_1's ndcg@20: 0.826228\n",
      "[200]\ttraining's ndcg@20: 0.904289\tvalid_1's ndcg@20: 0.826077\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's ndcg@20: 0.903344\tvalid_1's ndcg@20: 0.826364\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 1 orders recall = 0.7274785844684736\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2/5....\n",
      "before mean: 0.009842340724804607\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7175640, 173)\n",
      "cross_feature stacking, x_train...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "cross_feature stacking, x_val...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.381577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 37401\n",
      "[LightGBM] [Info] Number of data points in the train set: 7175640, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.884395\tvalid_1's ndcg@20: 0.820424\n",
      "[20]\ttraining's ndcg@20: 0.886117\tvalid_1's ndcg@20: 0.822149\n",
      "[30]\ttraining's ndcg@20: 0.887285\tvalid_1's ndcg@20: 0.823087\n",
      "[40]\ttraining's ndcg@20: 0.888081\tvalid_1's ndcg@20: 0.823602\n",
      "[50]\ttraining's ndcg@20: 0.889245\tvalid_1's ndcg@20: 0.824074\n",
      "[60]\ttraining's ndcg@20: 0.890449\tvalid_1's ndcg@20: 0.824656\n",
      "[70]\ttraining's ndcg@20: 0.891676\tvalid_1's ndcg@20: 0.825461\n",
      "[80]\ttraining's ndcg@20: 0.892778\tvalid_1's ndcg@20: 0.825775\n",
      "[90]\ttraining's ndcg@20: 0.893862\tvalid_1's ndcg@20: 0.82611\n",
      "[100]\ttraining's ndcg@20: 0.894998\tvalid_1's ndcg@20: 0.826297\n",
      "[110]\ttraining's ndcg@20: 0.896041\tvalid_1's ndcg@20: 0.826579\n",
      "[120]\ttraining's ndcg@20: 0.896915\tvalid_1's ndcg@20: 0.826672\n",
      "[130]\ttraining's ndcg@20: 0.897905\tvalid_1's ndcg@20: 0.826888\n",
      "[140]\ttraining's ndcg@20: 0.898839\tvalid_1's ndcg@20: 0.827047\n",
      "[150]\ttraining's ndcg@20: 0.899777\tvalid_1's ndcg@20: 0.827189\n",
      "[160]\ttraining's ndcg@20: 0.900579\tvalid_1's ndcg@20: 0.827427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[170]\ttraining's ndcg@20: 0.901317\tvalid_1's ndcg@20: 0.827325\n",
      "[180]\ttraining's ndcg@20: 0.902242\tvalid_1's ndcg@20: 0.827323\n",
      "[190]\ttraining's ndcg@20: 0.902976\tvalid_1's ndcg@20: 0.827465\n",
      "[200]\ttraining's ndcg@20: 0.903669\tvalid_1's ndcg@20: 0.827464\n",
      "[210]\ttraining's ndcg@20: 0.904279\tvalid_1's ndcg@20: 0.827559\n",
      "[220]\ttraining's ndcg@20: 0.904983\tvalid_1's ndcg@20: 0.827697\n",
      "[230]\ttraining's ndcg@20: 0.905653\tvalid_1's ndcg@20: 0.827723\n",
      "[240]\ttraining's ndcg@20: 0.906407\tvalid_1's ndcg@20: 0.82766\n",
      "[250]\ttraining's ndcg@20: 0.907184\tvalid_1's ndcg@20: 0.827645\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's ndcg@20: 0.905831\tvalid_1's ndcg@20: 0.827786\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 2 orders recall = 0.7321093845803479\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3/5....\n",
      "before mean: 0.009842684972505789\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7174000, 173)\n",
      "cross_feature stacking, x_train...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "cross_feature stacking, x_val...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.378692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 37370\n",
      "[LightGBM] [Info] Number of data points in the train set: 7174000, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.8848\tvalid_1's ndcg@20: 0.820833\n",
      "[20]\ttraining's ndcg@20: 0.88635\tvalid_1's ndcg@20: 0.822414\n",
      "[30]\ttraining's ndcg@20: 0.887351\tvalid_1's ndcg@20: 0.823419\n",
      "[40]\ttraining's ndcg@20: 0.888278\tvalid_1's ndcg@20: 0.823668\n",
      "[50]\ttraining's ndcg@20: 0.889556\tvalid_1's ndcg@20: 0.824595\n",
      "[60]\ttraining's ndcg@20: 0.89056\tvalid_1's ndcg@20: 0.825062\n",
      "[70]\ttraining's ndcg@20: 0.891806\tvalid_1's ndcg@20: 0.825192\n",
      "[80]\ttraining's ndcg@20: 0.892847\tvalid_1's ndcg@20: 0.825483\n",
      "[90]\ttraining's ndcg@20: 0.893936\tvalid_1's ndcg@20: 0.826276\n",
      "[100]\ttraining's ndcg@20: 0.895042\tvalid_1's ndcg@20: 0.826469\n",
      "[110]\ttraining's ndcg@20: 0.896099\tvalid_1's ndcg@20: 0.826775\n",
      "[120]\ttraining's ndcg@20: 0.89703\tvalid_1's ndcg@20: 0.82701\n",
      "[130]\ttraining's ndcg@20: 0.897895\tvalid_1's ndcg@20: 0.82711\n",
      "[140]\ttraining's ndcg@20: 0.898862\tvalid_1's ndcg@20: 0.82729\n",
      "[150]\ttraining's ndcg@20: 0.899795\tvalid_1's ndcg@20: 0.82723\n",
      "[160]\ttraining's ndcg@20: 0.900658\tvalid_1's ndcg@20: 0.827374\n",
      "[170]\ttraining's ndcg@20: 0.901412\tvalid_1's ndcg@20: 0.827567\n",
      "[180]\ttraining's ndcg@20: 0.902128\tvalid_1's ndcg@20: 0.827618\n",
      "[190]\ttraining's ndcg@20: 0.902922\tvalid_1's ndcg@20: 0.827712\n",
      "[200]\ttraining's ndcg@20: 0.903593\tvalid_1's ndcg@20: 0.827843\n",
      "[210]\ttraining's ndcg@20: 0.904307\tvalid_1's ndcg@20: 0.827807\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's ndcg@20: 0.903267\tvalid_1's ndcg@20: 0.827872\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 3 orders recall = 0.7214448197421934\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4/5....\n",
      "before mean: 0.009876283562224624\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7201720, 173)\n",
      "cross_feature stacking, x_train...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "cross_feature stacking, x_val...\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.182184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 37381\n",
      "[LightGBM] [Info] Number of data points in the train set: 7201720, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.884408\tvalid_1's ndcg@20: 0.820491\n",
      "[20]\ttraining's ndcg@20: 0.886131\tvalid_1's ndcg@20: 0.821393\n",
      "[30]\ttraining's ndcg@20: 0.88722\tvalid_1's ndcg@20: 0.822595\n",
      "[40]\ttraining's ndcg@20: 0.888018\tvalid_1's ndcg@20: 0.822963\n",
      "[50]\ttraining's ndcg@20: 0.889126\tvalid_1's ndcg@20: 0.823606\n",
      "[60]\ttraining's ndcg@20: 0.890364\tvalid_1's ndcg@20: 0.824131\n",
      "[70]\ttraining's ndcg@20: 0.891555\tvalid_1's ndcg@20: 0.824758\n",
      "[80]\ttraining's ndcg@20: 0.892583\tvalid_1's ndcg@20: 0.824897\n",
      "[90]\ttraining's ndcg@20: 0.893741\tvalid_1's ndcg@20: 0.825089\n",
      "[100]\ttraining's ndcg@20: 0.894855\tvalid_1's ndcg@20: 0.825403\n",
      "[110]\ttraining's ndcg@20: 0.895974\tvalid_1's ndcg@20: 0.825868\n",
      "[120]\ttraining's ndcg@20: 0.896983\tvalid_1's ndcg@20: 0.82628\n",
      "[130]\ttraining's ndcg@20: 0.897923\tvalid_1's ndcg@20: 0.826568\n",
      "[140]\ttraining's ndcg@20: 0.898821\tvalid_1's ndcg@20: 0.826712\n",
      "[150]\ttraining's ndcg@20: 0.899653\tvalid_1's ndcg@20: 0.826896\n",
      "[160]\ttraining's ndcg@20: 0.90054\tvalid_1's ndcg@20: 0.826873\n",
      "[170]\ttraining's ndcg@20: 0.901366\tvalid_1's ndcg@20: 0.827071\n",
      "[180]\ttraining's ndcg@20: 0.902204\tvalid_1's ndcg@20: 0.827077\n",
      "[190]\ttraining's ndcg@20: 0.902891\tvalid_1's ndcg@20: 0.827281\n",
      "[200]\ttraining's ndcg@20: 0.90352\tvalid_1's ndcg@20: 0.827331\n",
      "[210]\ttraining's ndcg@20: 0.9042\tvalid_1's ndcg@20: 0.827126\n",
      "[220]\ttraining's ndcg@20: 0.904971\tvalid_1's ndcg@20: 0.827218\n",
      "Early stopping, best iteration is:\n",
      "[200]\ttraining's ndcg@20: 0.90352\tvalid_1's ndcg@20: 0.827331\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 4 orders recall = 0.7295813558121413\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "session = train['session']\n",
    "unique_session = session.unique()\n",
    "\n",
    "N_splits = 5\n",
    "kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}/{N_splits}....')\n",
    "    # session単位で分割してKFoldする\n",
    "    tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "    is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "    del tr_groups, va_groups\n",
    "    gc.collect()\n",
    "    # is_ir, is_va=Trueのindexを取得\n",
    "    trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "    del is_tr, is_va\n",
    "    gc.collect()\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "    query_list_train = x_train['session'].value_counts()\n",
    "    query_list_train = query_list_train.sort_index()\n",
    "\n",
    "    query_list_valid = x_val['session'].value_counts()\n",
    "    query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "    # memory節約のため, under sampling後にfeature追加\n",
    "    print('add session features....')\n",
    "    x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "    print('add aid features....')\n",
    "    x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "    print('add interactive features....')\n",
    "    x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "    print('remove features....')\n",
    "    x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "    print('remove id from features....')\n",
    "    x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        print('cross_feature stacking, x_train...')\n",
    "        x_train = add_cross_stacking_feature(x_train, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        print('cross_feature stacking, x_val...')\n",
    "        x_val = add_cross_stacking_feature(x_val, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        #num_boost_round = 100,\n",
    "        num_boost_round = 2000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 20,\n",
    "        verbose_eval = 10,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "    # Save best model\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}_stack.pkl')\n",
    "    else:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    # tmp recall for each fold\n",
    "    df = pd.DataFrame(val_pred, columns=[\"score\"])\n",
    "    tmp = train[['session', 'aid']].iloc[val_ind].reset_index(drop=True)\n",
    "    pred_df = pd.concat([tmp, df], axis=1)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "    pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "    pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "    pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "    pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "    pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "    pred_df = pred_df.drop(['aid'],axis=1)\n",
    "\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "    test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "    # foldごとのreallなのでinnter\n",
    "    test_labels = test_labels.merge(sub, how='inner', on=['session']) \n",
    "    test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    print(f'fold {fold} {TYPE_MODE} recall =',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_orders</td>\n",
       "      <td>11830 1732105 588923 876129 884502 1157882 571...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098530_orders</td>\n",
       "      <td>409236 1603001 264500 364155 963957 254154 210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098531_orders</td>\n",
       "      <td>1365569 1557766 1309633 1553691 1449555 396199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098533_orders</td>\n",
       "      <td>1074173 1309900 1165015 765030 833149 1622419 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098534_orders</td>\n",
       "      <td>223062 908024 1607945 1342293 1449202 530377 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133618</th>\n",
       "      <td>12899159_orders</td>\n",
       "      <td>1512596 1383649 1131172 314450 1178395 1616589...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133619</th>\n",
       "      <td>12899329_orders</td>\n",
       "      <td>1333457 1470364 356732 1667554 931182 977011 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133620</th>\n",
       "      <td>12899337_orders</td>\n",
       "      <td>558573 1662401 742581 1223508 1581401 1600937 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133621</th>\n",
       "      <td>12899373_orders</td>\n",
       "      <td>1766353 487949 461938 1662986 1763592 516917 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133622</th>\n",
       "      <td>12899525_orders</td>\n",
       "      <td>996393 1488793 127479 1599360 405179 1123744 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133623 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           session_type                                             labels\n",
       "0       11098528_orders  11830 1732105 588923 876129 884502 1157882 571...\n",
       "1       11098530_orders  409236 1603001 264500 364155 963957 254154 210...\n",
       "2       11098531_orders  1365569 1557766 1309633 1553691 1449555 396199...\n",
       "3       11098533_orders  1074173 1309900 1165015 765030 833149 1622419 ...\n",
       "4       11098534_orders  223062 908024 1607945 1342293 1449202 530377 1...\n",
       "...                 ...                                                ...\n",
       "133618  12899159_orders  1512596 1383649 1131172 314450 1178395 1616589...\n",
       "133619  12899329_orders  1333457 1470364 356732 1667554 931182 977011 1...\n",
       "133620  12899337_orders  558573 1662401 742581 1223508 1581401 1600937 ...\n",
       "133621  12899373_orders  1766353 487949 461938 1662986 1763592 516917 1...\n",
       "133622  12899525_orders  996393 1488793 127479 1599360 405179 1123744 1...\n",
       "\n",
       "[133623 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "if CROSS_TARGET_STACKING:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}_stack.parquet')\n",
    "else:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}.parquet')\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders recall = 0.6647973367634527\n"
     ]
    }
   ],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# ranker model, fold0を factor=1.111369 で割ればそれっぽい値が出る, each foldは session inner joinなので高めに出る\n",
    "# num=100, lr=0.1, no feature add, valid_1's ndcg@50: 0.843586, fold 0 orders recall = 0.7263385039885385, orders recall = 0.6535526311589739\n",
    "# add aid feature, valid_1's ndcg@50: 0.84962, fold 0 orders recall = 0.7305304490864389, (orders recall = 0.657324?)\n",
    "# add aid + session feature, valid_1's ndcg@50: 0.849762, fold 0 orders recall = 0.7302651361055592, orders recall = 0.6575806806829172\n",
    "# all valid_1's ndcg@50: 0.848664, fold 0 orders recall = 0.730990324919964, orders recall = 0.6579892308723504\n",
    "\n",
    "\n",
    "# hypter paramやりなおし、regularization param大幅変更 num=100, lr=0.1\n",
    "# no add: valid_1's ndcg@20: 0.835401, fold 0 orders recall = 0.7261793162000106, orders recall = 0.6535877409408783  -> order,carts差し替えPB = 0.581\n",
    "#                                                                                 carts recall = 0.41837386076234817\n",
    "# sessionのみ: valid_1's ndcg@20: 0.836164, fold 0 orders recall = 0.7268514424182394, orders recall = 0.6545069788671031 -> order,carts差し替えPB = 0.583\n",
    "#                                                                                      carts recall = 0.4196106730132077\n",
    "# aidのみ: valid_1's ndcg@20: 0.842205, fold 0 orders recall = 0.7302828236376179, orders recall = 0.6575838724812721 -> order,carts差し替えPB = 0.586 (55, 1/19)\n",
    "#                                                                                  carts recall = 0.42261163401459195                              \n",
    "# aid+session: valid_1's ndcg@20: 0.843169, fold 0 orders recall = 0.7309018872596706, orders recall = 0.6582030813621319 -> order,carts差し替えPB = 0.587 (54, 1/19)\n",
    "#                                                                                      carts recall = 0.42347896378377814\n",
    "# all: valid_1's ndcg@20: 0.843514, fold 0 orders recall = 0.731184887772609, orders recall = 0.6584137400535583 -> order,carts差し替えPB = 0.586 下がったけどブレ？\n",
    "#                                                                             carts recall = 0.42349804503870025\n",
    "# num=1000, lr=0.05 [281] valid_1's ndcg@20: 0.844737, fold 0 orders recall = 0.7315386384137821, orders recall = 0.6586627003252442 -> PB = 0.587 (53, 1/19)\n",
    "#                                                                                                 carts recall = 0.4242057861303562\n",
    "#                                                                                                 clicks recall = 0.536971\n",
    "# candidate add, mean 60 -> 120\n",
    "# lr=0.1, [100] valid_1's ndcg@20: 0.832289 fold 0 orders recall = 0.7260696029689797, orders recall = 0.6619853624127442-> PB = 0.592 \n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.833279,fold 0 orders recall = 0.7268398571528605, orders recall = 0.6622981586515291\n",
    "#                                                                                      carts recall = 0.4296682290166909\n",
    "#                                                                                      clicks recall = 0.54153778850196010\n",
    "# candidate add more\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.824422, fold 0 orders recall = 0.7264196759981961, orders recall = 0.6627609694129963-> PB = 0.592\n",
    "#                                                                                       carts recall = 0.43018862687820264\n",
    "#                                                                                       clicks recall = 0.5425910292822583\n",
    "# stacking test, be careful to leak..\n",
    "# lr=0.1,  valid_1's ndcg@20: 0.825827 [100] fold 0 orders recall = 0.7276858500711139, orders recall = 0.6645771026769612\n",
    "# lr=0.05 valid_1's ndcg@20: 0.826031 [131] fold 0 orders recall = 0.728258230131474, orders recall = 0.6647973367634527\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
