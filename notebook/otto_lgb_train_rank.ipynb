{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7228,
     "status": "ok",
     "timestamp": 1674477402817,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e",
    "outputId": "9e930ba3-d13a-4ee8-f28a-a681a3840d33"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    notebook_path = base_path + '/otto/notebook'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'\n",
    "    notebook_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1674477405015,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "sys.path.append(f\"{notebook_path}/../src/\")\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18480,
     "status": "ok",
     "timestamp": 1674477423492,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_0.parquet') # 0.592\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_0_ver2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "#DEBUG_MODE = True\n",
    "\n",
    "OPTUNA_FLAG = False\n",
    "#OPTUNA_FLAG = True\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL_ID = ['session','aid']\n",
    "\n",
    "TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "#TYPE_MODE = 'orders'\n",
    "IGNORE_COL_TARGET = ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    #pos_neg_ratio = 1/39\n",
    "    pos_neg_ratio = 1/29 # 3.3%\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "\n",
    "# usually same as target \n",
    "# change this value if you want to calculate target prediction feature\n",
    "remove_negative_session_target = target\n",
    "#remove_negative_session_target = 'y_orders'\n",
    "\n",
    "# use prediction score from other target or not\n",
    "# only apply for order and cart\n",
    "USE_CROSS_TARGET = False\n",
    "\n",
    "session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df, _target):\n",
    "    true_df = df.groupby('session')[_target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio, random_state=0).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "    # sessionの順番がばらばらになるので再びsort\n",
    "    tmp = pd.concat([after_x, after_y], axis=1).sort_values('session')\n",
    "    after_y = tmp[target]\n",
    "    after_x = tmp.drop(target , axis=1)\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XBANiP7Wfvwt"
   },
   "outputs": [],
   "source": [
    "# importanceが極端に低いものを削る (18件)\n",
    "def remove_features(df):\n",
    "    DROP_COL = ['session_type_mean']\n",
    "    return df.drop(DROP_COL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5102,
     "status": "ok",
     "timestamp": 1674477428581,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = fe.reduce_memory(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10653,
     "status": "ok",
     "timestamp": 1674477439212,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eUrB2zTWf4ZK",
    "outputId": "26a3ff7f-f3fd-4845-d868-18a0146fc11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sum: 1105625\n",
      "target mean: 0.007330202023516978\n"
     ]
    }
   ],
   "source": [
    "train = remove_negative_session(train, remove_negative_session_target)\n",
    "print('target sum:', train[target].sum())\n",
    "print('target mean:', train[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    session = train['session']\n",
    "    unique_session = session.unique()\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [20],\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    N_splits = 5\n",
    "    kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}/{N_splits}....')\n",
    "        # session単位で分割してKFoldする\n",
    "        tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "        is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "        del tr_groups, va_groups\n",
    "        gc.collect()\n",
    "        # is_ir, is_va=Trueのindexを取得\n",
    "        trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "        del is_tr, is_va\n",
    "        gc.collect()\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        # under sampling\n",
    "        x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "        # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "        query_list_train = x_train['session'].value_counts()\n",
    "        query_list_train = query_list_train.sort_index()\n",
    "\n",
    "        query_list_valid = x_val['session'].value_counts()\n",
    "        query_list_valid = query_list_valid.sort_index()\n",
    "        \n",
    "\n",
    "        # memory節約のため, under sampling後にfeature追加\n",
    "        print('add session features....')\n",
    "        x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "        print('add aid features....')\n",
    "        x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "        print('add interactive features....')\n",
    "        x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "        print('remove features....')\n",
    "        x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "        print('remove id from features....')\n",
    "        x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 20,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439214,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "params = {'objective': 'lambdarank', \n",
    "          'metric': 'ndcg', \n",
    "          'ndcg_eval_at': [20], \n",
    "          'boosting': 'gbdt', \n",
    "          'seed': 42, \n",
    "          'n_jobs': -1, \n",
    "          'learning_rate': 0.1, \n",
    "          #'learning_rate': 0.05, \n",
    "          'feature_pre_filter': False, \n",
    "          'lambda_l1': 1.7510743847807332e-08, \n",
    "          'lambda_l2': 3.773149139134113e-07, \n",
    "          'num_leaves': 108, \n",
    "          'feature_fraction': 0.4, \n",
    "          'bagging_fraction': 1.0, \n",
    "          'bagging_freq': 0, \n",
    "          'min_child_samples': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "9cc7f1c4-d87f-47b3-c476-5f23190d3cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0/5....\n",
      "before mean: 0.007329588881577991\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.03333333333333333\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (26535000, 173)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.169647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36421\n",
      "[LightGBM] [Info] Number of data points in the train set: 26535000, number of used features: 173\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.77251\tvalid_1's ndcg@20: 0.587352\n",
      "[20]\ttraining's ndcg@20: 0.774247\tvalid_1's ndcg@20: 0.589292\n",
      "[30]\ttraining's ndcg@20: 0.775355\tvalid_1's ndcg@20: 0.590661\n",
      "[40]\ttraining's ndcg@20: 0.776363\tvalid_1's ndcg@20: 0.591685\n",
      "[50]\ttraining's ndcg@20: 0.777153\tvalid_1's ndcg@20: 0.592623\n",
      "[60]\ttraining's ndcg@20: 0.777869\tvalid_1's ndcg@20: 0.592998\n",
      "[70]\ttraining's ndcg@20: 0.778561\tvalid_1's ndcg@20: 0.593621\n",
      "[80]\ttraining's ndcg@20: 0.779227\tvalid_1's ndcg@20: 0.594072\n",
      "[90]\ttraining's ndcg@20: 0.779847\tvalid_1's ndcg@20: 0.594309\n",
      "[100]\ttraining's ndcg@20: 0.780404\tvalid_1's ndcg@20: 0.594589\n",
      "[110]\ttraining's ndcg@20: 0.781\tvalid_1's ndcg@20: 0.594814\n",
      "[120]\ttraining's ndcg@20: 0.781496\tvalid_1's ndcg@20: 0.595001\n",
      "[130]\ttraining's ndcg@20: 0.781974\tvalid_1's ndcg@20: 0.595034\n",
      "[140]\ttraining's ndcg@20: 0.782424\tvalid_1's ndcg@20: 0.595053\n",
      "[150]\ttraining's ndcg@20: 0.782934\tvalid_1's ndcg@20: 0.5951\n",
      "[160]\ttraining's ndcg@20: 0.783306\tvalid_1's ndcg@20: 0.595112\n",
      "[170]\ttraining's ndcg@20: 0.783686\tvalid_1's ndcg@20: 0.595173\n",
      "[180]\ttraining's ndcg@20: 0.784078\tvalid_1's ndcg@20: 0.595143\n",
      "[190]\ttraining's ndcg@20: 0.78438\tvalid_1's ndcg@20: 0.595089\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's ndcg@20: 0.783686\tvalid_1's ndcg@20: 0.595173\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 0 clicks recall = 0.86191972866026\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1/5....\n",
      "before mean: 0.007329680658036769\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.03333333333333333\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (26535000, 173)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 3.847343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36429\n",
      "[LightGBM] [Info] Number of data points in the train set: 26535000, number of used features: 173\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.772458\tvalid_1's ndcg@20: 0.586609\n",
      "[20]\ttraining's ndcg@20: 0.774266\tvalid_1's ndcg@20: 0.588501\n",
      "[30]\ttraining's ndcg@20: 0.775471\tvalid_1's ndcg@20: 0.590167\n",
      "[40]\ttraining's ndcg@20: 0.776539\tvalid_1's ndcg@20: 0.591131\n",
      "[50]\ttraining's ndcg@20: 0.77735\tvalid_1's ndcg@20: 0.591969\n",
      "[60]\ttraining's ndcg@20: 0.778071\tvalid_1's ndcg@20: 0.592445\n",
      "[70]\ttraining's ndcg@20: 0.778756\tvalid_1's ndcg@20: 0.593099\n",
      "[80]\ttraining's ndcg@20: 0.77934\tvalid_1's ndcg@20: 0.593509\n",
      "[90]\ttraining's ndcg@20: 0.779906\tvalid_1's ndcg@20: 0.593809\n",
      "[100]\ttraining's ndcg@20: 0.78049\tvalid_1's ndcg@20: 0.594101\n",
      "[110]\ttraining's ndcg@20: 0.781042\tvalid_1's ndcg@20: 0.594197\n",
      "[120]\ttraining's ndcg@20: 0.78154\tvalid_1's ndcg@20: 0.594254\n",
      "[130]\ttraining's ndcg@20: 0.782073\tvalid_1's ndcg@20: 0.594465\n",
      "[140]\ttraining's ndcg@20: 0.78255\tvalid_1's ndcg@20: 0.59454\n",
      "[150]\ttraining's ndcg@20: 0.782985\tvalid_1's ndcg@20: 0.594647\n",
      "[160]\ttraining's ndcg@20: 0.783419\tvalid_1's ndcg@20: 0.594641\n",
      "[170]\ttraining's ndcg@20: 0.783806\tvalid_1's ndcg@20: 0.594668\n",
      "[180]\ttraining's ndcg@20: 0.784194\tvalid_1's ndcg@20: 0.59465\n",
      "[190]\ttraining's ndcg@20: 0.784524\tvalid_1's ndcg@20: 0.594652\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's ndcg@20: 0.783997\tvalid_1's ndcg@20: 0.594696\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 1 clicks recall = 0.8619830412662521\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2/5....\n",
      "before mean: 0.007332220576638156\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.03333333333333333\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (26535000, 173)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.281166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36400\n",
      "[LightGBM] [Info] Number of data points in the train set: 26535000, number of used features: 173\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.772456\tvalid_1's ndcg@20: 0.586072\n",
      "[20]\ttraining's ndcg@20: 0.774336\tvalid_1's ndcg@20: 0.58861\n",
      "[30]\ttraining's ndcg@20: 0.775412\tvalid_1's ndcg@20: 0.589665\n",
      "[40]\ttraining's ndcg@20: 0.776469\tvalid_1's ndcg@20: 0.590719\n",
      "[50]\ttraining's ndcg@20: 0.777274\tvalid_1's ndcg@20: 0.591573\n",
      "[60]\ttraining's ndcg@20: 0.777918\tvalid_1's ndcg@20: 0.592103\n",
      "[70]\ttraining's ndcg@20: 0.778608\tvalid_1's ndcg@20: 0.592683\n",
      "[80]\ttraining's ndcg@20: 0.779199\tvalid_1's ndcg@20: 0.592994\n",
      "[90]\ttraining's ndcg@20: 0.779746\tvalid_1's ndcg@20: 0.593306\n",
      "[100]\ttraining's ndcg@20: 0.780339\tvalid_1's ndcg@20: 0.593506\n",
      "[110]\ttraining's ndcg@20: 0.78092\tvalid_1's ndcg@20: 0.593734\n",
      "[120]\ttraining's ndcg@20: 0.781506\tvalid_1's ndcg@20: 0.593864\n",
      "[130]\ttraining's ndcg@20: 0.782023\tvalid_1's ndcg@20: 0.59401\n",
      "[140]\ttraining's ndcg@20: 0.78247\tvalid_1's ndcg@20: 0.59401\n",
      "[150]\ttraining's ndcg@20: 0.782901\tvalid_1's ndcg@20: 0.594021\n",
      "[160]\ttraining's ndcg@20: 0.783303\tvalid_1's ndcg@20: 0.594142\n",
      "[170]\ttraining's ndcg@20: 0.783686\tvalid_1's ndcg@20: 0.594157\n",
      "[180]\ttraining's ndcg@20: 0.784095\tvalid_1's ndcg@20: 0.594222\n",
      "[190]\ttraining's ndcg@20: 0.784486\tvalid_1's ndcg@20: 0.594286\n",
      "[200]\ttraining's ndcg@20: 0.78484\tvalid_1's ndcg@20: 0.594314\n",
      "[210]\ttraining's ndcg@20: 0.785214\tvalid_1's ndcg@20: 0.594335\n",
      "[220]\ttraining's ndcg@20: 0.785536\tvalid_1's ndcg@20: 0.594315\n",
      "[230]\ttraining's ndcg@20: 0.785886\tvalid_1's ndcg@20: 0.594342\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttraining's ndcg@20: 0.785385\tvalid_1's ndcg@20: 0.594365\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 2 clicks recall = 0.8618971170152628\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3/5....\n",
      "before mean: 0.00732829982010826\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.03333333333333333\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (26535000, 173)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.353681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36402\n",
      "[LightGBM] [Info] Number of data points in the train set: 26535000, number of used features: 173\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.772332\tvalid_1's ndcg@20: 0.586644\n",
      "[20]\ttraining's ndcg@20: 0.774192\tvalid_1's ndcg@20: 0.58891\n",
      "[30]\ttraining's ndcg@20: 0.775323\tvalid_1's ndcg@20: 0.590262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\ttraining's ndcg@20: 0.776362\tvalid_1's ndcg@20: 0.59148\n",
      "[50]\ttraining's ndcg@20: 0.777195\tvalid_1's ndcg@20: 0.592333\n",
      "[60]\ttraining's ndcg@20: 0.777898\tvalid_1's ndcg@20: 0.5929\n",
      "[70]\ttraining's ndcg@20: 0.77853\tvalid_1's ndcg@20: 0.59353\n",
      "[80]\ttraining's ndcg@20: 0.779194\tvalid_1's ndcg@20: 0.593925\n",
      "[90]\ttraining's ndcg@20: 0.779757\tvalid_1's ndcg@20: 0.594258\n",
      "[100]\ttraining's ndcg@20: 0.780349\tvalid_1's ndcg@20: 0.594462\n",
      "[110]\ttraining's ndcg@20: 0.780924\tvalid_1's ndcg@20: 0.59455\n",
      "[120]\ttraining's ndcg@20: 0.781457\tvalid_1's ndcg@20: 0.594729\n",
      "[130]\ttraining's ndcg@20: 0.781957\tvalid_1's ndcg@20: 0.594839\n",
      "[140]\ttraining's ndcg@20: 0.782405\tvalid_1's ndcg@20: 0.594824\n",
      "[150]\ttraining's ndcg@20: 0.782838\tvalid_1's ndcg@20: 0.594912\n",
      "[160]\ttraining's ndcg@20: 0.783211\tvalid_1's ndcg@20: 0.59504\n",
      "[170]\ttraining's ndcg@20: 0.783617\tvalid_1's ndcg@20: 0.595055\n",
      "[180]\ttraining's ndcg@20: 0.783976\tvalid_1's ndcg@20: 0.595074\n",
      "[190]\ttraining's ndcg@20: 0.784391\tvalid_1's ndcg@20: 0.59506\n",
      "[200]\ttraining's ndcg@20: 0.784771\tvalid_1's ndcg@20: 0.59512\n",
      "[210]\ttraining's ndcg@20: 0.785128\tvalid_1's ndcg@20: 0.595125\n",
      "[220]\ttraining's ndcg@20: 0.785409\tvalid_1's ndcg@20: 0.595183\n",
      "[230]\ttraining's ndcg@20: 0.78565\tvalid_1's ndcg@20: 0.595192\n",
      "[240]\ttraining's ndcg@20: 0.78602\tvalid_1's ndcg@20: 0.595234\n",
      "[250]\ttraining's ndcg@20: 0.786412\tvalid_1's ndcg@20: 0.595247\n",
      "[260]\ttraining's ndcg@20: 0.786691\tvalid_1's ndcg@20: 0.595232\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's ndcg@20: 0.786255\tvalid_1's ndcg@20: 0.595286\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 3 clicks recall = 0.8618790276992652\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4/5....\n",
      "before mean: 0.007331221460816785\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.03333333333333333\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (26535000, 173)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.576667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 36383\n",
      "[LightGBM] [Info] Number of data points in the train set: 26535000, number of used features: 173\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.772392\tvalid_1's ndcg@20: 0.585786\n",
      "[20]\ttraining's ndcg@20: 0.774215\tvalid_1's ndcg@20: 0.587542\n",
      "[30]\ttraining's ndcg@20: 0.775374\tvalid_1's ndcg@20: 0.588831\n",
      "[40]\ttraining's ndcg@20: 0.776445\tvalid_1's ndcg@20: 0.590144\n",
      "[50]\ttraining's ndcg@20: 0.777244\tvalid_1's ndcg@20: 0.591064\n",
      "[60]\ttraining's ndcg@20: 0.777955\tvalid_1's ndcg@20: 0.591652\n",
      "[70]\ttraining's ndcg@20: 0.778686\tvalid_1's ndcg@20: 0.592142\n",
      "[80]\ttraining's ndcg@20: 0.77931\tvalid_1's ndcg@20: 0.592536\n",
      "[90]\ttraining's ndcg@20: 0.779898\tvalid_1's ndcg@20: 0.592841\n",
      "[100]\ttraining's ndcg@20: 0.78047\tvalid_1's ndcg@20: 0.592982\n",
      "[110]\ttraining's ndcg@20: 0.781058\tvalid_1's ndcg@20: 0.593137\n",
      "[120]\ttraining's ndcg@20: 0.781561\tvalid_1's ndcg@20: 0.593284\n",
      "[130]\ttraining's ndcg@20: 0.782044\tvalid_1's ndcg@20: 0.593417\n",
      "[140]\ttraining's ndcg@20: 0.782475\tvalid_1's ndcg@20: 0.593366\n",
      "[150]\ttraining's ndcg@20: 0.782894\tvalid_1's ndcg@20: 0.59343\n",
      "[160]\ttraining's ndcg@20: 0.783361\tvalid_1's ndcg@20: 0.593573\n",
      "[170]\ttraining's ndcg@20: 0.78376\tvalid_1's ndcg@20: 0.593572\n",
      "[180]\ttraining's ndcg@20: 0.784155\tvalid_1's ndcg@20: 0.593595\n",
      "[190]\ttraining's ndcg@20: 0.78454\tvalid_1's ndcg@20: 0.593659\n",
      "[200]\ttraining's ndcg@20: 0.784884\tvalid_1's ndcg@20: 0.593606\n",
      "Early stopping, best iteration is:\n",
      "[187]\ttraining's ndcg@20: 0.784437\tvalid_1's ndcg@20: 0.593676\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 4 clicks recall = 0.8600067834934991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "session = train['session']\n",
    "unique_session = session.unique()\n",
    "\n",
    "N_splits = 5\n",
    "kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}/{N_splits}....')\n",
    "    # session単位で分割してKFoldする\n",
    "    tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "    is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "    del tr_groups, va_groups\n",
    "    gc.collect()\n",
    "    # is_ir, is_va=Trueのindexを取得\n",
    "    trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "    del is_tr, is_va\n",
    "    gc.collect()\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "    query_list_train = x_train['session'].value_counts()\n",
    "    query_list_train = query_list_train.sort_index()\n",
    "\n",
    "    query_list_valid = x_val['session'].value_counts()\n",
    "    query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "    # memory節約のため, under sampling後にfeature追加\n",
    "    print('add session features....')\n",
    "    x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "    print('add aid features....')\n",
    "    x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "    print('add interactive features....')\n",
    "    x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "    print('remove features....')\n",
    "    x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "    print('remove id from features....')\n",
    "    x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        #num_boost_round = 100,\n",
    "        num_boost_round = 2000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 20,\n",
    "        verbose_eval = 10,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    # tmp recall for each fold\n",
    "    df = pd.DataFrame(val_pred, columns=[\"score\"])\n",
    "    tmp = train[['session', 'aid']].iloc[val_ind].reset_index(drop=True)\n",
    "    pred_df = pd.concat([tmp, df], axis=1)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "    pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "    pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "    pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "    pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "    pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "    pred_df = pred_df.drop(['aid'],axis=1)\n",
    "\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "    test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "    # foldごとのreallなのでinnter\n",
    "    test_labels = test_labels.merge(sub, how='inner', on=['session']) \n",
    "    test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    print(f'fold {fold} {TYPE_MODE} recall =',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 217742 295362 1544564 1383767 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098534_clicks</td>\n",
       "      <td>223062 908024 1342293 1607945 1300062 530377 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098535_clicks</td>\n",
       "      <td>745365 803918 1750442 767201 896972 85930 9076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098537_clicks</td>\n",
       "      <td>358965 1409748 336024 1723620 294268 1189975 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098538_clicks</td>\n",
       "      <td>1263747 703265 1550143 1711586 717871 351587 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105620</th>\n",
       "      <td>12899774_clicks</td>\n",
       "      <td>33035 1539309 270852 771913 819288 1226691 218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105621</th>\n",
       "      <td>12899775_clicks</td>\n",
       "      <td>1743151 1760714 1255910 1163166 1022572 310546...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105622</th>\n",
       "      <td>12899776_clicks</td>\n",
       "      <td>548599 1440959 1401030 1607333 1144446 1150130...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105623</th>\n",
       "      <td>12899777_clicks</td>\n",
       "      <td>384045 1308634 1688215 395762 703474 1486067 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105624</th>\n",
       "      <td>12899778_clicks</td>\n",
       "      <td>561560 1167224 1175618 32070 566042 13942 1340...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105625 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098529_clicks  1105029 459126 217742 295362 1544564 1383767 5...\n",
       "1        11098534_clicks  223062 908024 1342293 1607945 1300062 530377 1...\n",
       "2        11098535_clicks  745365 803918 1750442 767201 896972 85930 9076...\n",
       "3        11098537_clicks  358965 1409748 336024 1723620 294268 1189975 4...\n",
       "4        11098538_clicks  1263747 703265 1550143 1711586 717871 351587 1...\n",
       "...                  ...                                                ...\n",
       "1105620  12899774_clicks  33035 1539309 270852 771913 819288 1226691 218...\n",
       "1105621  12899775_clicks  1743151 1760714 1255910 1163166 1022572 310546...\n",
       "1105622  12899776_clicks  548599 1440959 1401030 1607333 1144446 1150130...\n",
       "1105623  12899777_clicks  384045 1308634 1688215 395762 703474 1486067 2...\n",
       "1105624  12899778_clicks  561560 1167224 1175618 32070 566042 13942 1340...\n",
       "\n",
       "[1105625 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "if (not USE_CROSS_TARGET) and target != remove_negative_session_target:\n",
    "    df.to_csv(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}_{remove_negative_session_target}_session.csv', index = False)\n",
    "    \n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "if (not USE_CROSS_TARGET) and target == remove_negative_session_target:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}.parquet')\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5425910292822583\n"
     ]
    }
   ],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# ranker model, fold0を factor=1.111369 で割ればそれっぽい値が出る, each foldは session inner joinなので高めに出る\n",
    "# num=100, lr=0.1, no feature add, valid_1's ndcg@50: 0.843586, fold 0 orders recall = 0.7263385039885385, orders recall = 0.6535526311589739\n",
    "# add aid feature, valid_1's ndcg@50: 0.84962, fold 0 orders recall = 0.7305304490864389, (orders recall = 0.657324?)\n",
    "# add aid + session feature, valid_1's ndcg@50: 0.849762, fold 0 orders recall = 0.7302651361055592, orders recall = 0.6575806806829172\n",
    "# all valid_1's ndcg@50: 0.848664, fold 0 orders recall = 0.730990324919964, orders recall = 0.6579892308723504\n",
    "\n",
    "\n",
    "# hypter paramやりなおし、regularization param大幅変更 num=100, lr=0.1\n",
    "# no add: valid_1's ndcg@20: 0.835401, fold 0 orders recall = 0.7261793162000106, orders recall = 0.6535877409408783  -> order,carts差し替えPB = 0.581\n",
    "#                                                                                 carts recall = 0.41837386076234817\n",
    "# sessionのみ: valid_1's ndcg@20: 0.836164, fold 0 orders recall = 0.7268514424182394, orders recall = 0.6545069788671031 -> order,carts差し替えPB = 0.583\n",
    "#                                                                                      carts recall = 0.4196106730132077\n",
    "# aidのみ: valid_1's ndcg@20: 0.842205, fold 0 orders recall = 0.7302828236376179, orders recall = 0.6575838724812721 -> order,carts差し替えPB = 0.586 (55, 1/19)\n",
    "#                                                                                  carts recall = 0.42261163401459195                              \n",
    "# aid+session: valid_1's ndcg@20: 0.843169, fold 0 orders recall = 0.7309018872596706, orders recall = 0.6582030813621319 -> order,carts差し替えPB = 0.587 (54, 1/19)\n",
    "#                                                                                      carts recall = 0.42347896378377814\n",
    "# all: valid_1's ndcg@20: 0.843514, fold 0 orders recall = 0.731184887772609, orders recall = 0.6584137400535583 -> order,carts差し替えPB = 0.586 下がったけどブレ？\n",
    "#                                                                             carts recall = 0.42349804503870025\n",
    "# num=1000, lr=0.05 [281] valid_1's ndcg@20: 0.844737, fold 0 orders recall = 0.7315386384137821, orders recall = 0.6586627003252442 -> PB = 0.587 (53, 1/19)\n",
    "#                                                                                                 carts recall = 0.4242057861303562\n",
    "#                                                                                                 clicks recall = 0.536971\n",
    "# candidate add, mean 60 -> 120\n",
    "# lr=0.1, [100] valid_1's ndcg@20: 0.832289 fold 0 orders recall = 0.7260696029689797, orders recall = 0.6619853624127442\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.833279,fold 0 orders recall = 0.7268398571528605, orders recall = 0.6622981586515291\n",
    "#                                                                                      carts recall = 0.4296682290166909\n",
    "#                                                                                      clicks recall = 0.54153778850196010\n",
    "# candidate add more\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.824422, fold 0 orders recall = 0.7264196759981961, orders recall = 0.6627609694129963\n",
    "#                                                                                       carts recall = 0.43018862687820264\n",
    "#                                                                                       clicks recall = 0.5425910292822583\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
