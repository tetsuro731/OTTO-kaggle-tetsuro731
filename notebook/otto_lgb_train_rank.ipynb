{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7228,
     "status": "ok",
     "timestamp": 1674477402817,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e",
    "outputId": "9e930ba3-d13a-4ee8-f28a-a681a3840d33"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    notebook_path = base_path + '/otto/notebook'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'\n",
    "    notebook_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1674477405015,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "sys.path.append(f\"{notebook_path}/../src/\")\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18480,
     "status": "ok",
     "timestamp": 1674477423492,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_0.parquet') # 0.592\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_0_ver2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "OPTUNA_FLAG = False\n",
    "CROSS_TARGET_STACKING = False\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL_ID = ['session','aid']\n",
    "\n",
    "#TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "TYPE_MODE = 'orders'\n",
    "IGNORE_COL_TARGET = ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    #pos_neg_ratio = 1/39\n",
    "    #pos_neg_ratio = 1/29 # 3.3%\n",
    "    pos_neg_ratio = 1/19 # 5%\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'orders']\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['orders', 'clicks']\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'clicks']\n",
    "\n",
    "session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df, _target):\n",
    "    true_df = df.groupby('session')[_target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio, random_state=0).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "    # sessionの順番がばらばらになるので再びsort\n",
    "    tmp = pd.concat([after_x, after_y], axis=1).sort_values('session')\n",
    "    after_y = tmp[target]\n",
    "    after_x = tmp.drop(target , axis=1)\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y\n",
    "\n",
    "# dataframe, target_list, number of split data, number of fold\n",
    "def add_cross_stacking_feature(df, cross_target_list, n_div, n_fold, base_path):\n",
    "    # split data in order to save memory\n",
    "    Nrow = df.shape[0]\n",
    "    n = int(Nrow // n_div) + 1\n",
    "    df_list = []\n",
    "    for i in range(n_div):\n",
    "        tmp = df.iloc[i*n : (i+1)*n, :]\n",
    "        df_list.append(tmp)\n",
    "    \n",
    "    # initialization\n",
    "    res_df = pd.DataFrame(columns=[], index = [])\n",
    "    for i, t in enumerate(cross_target_list):\n",
    "        df_pred = np.zeros(Nrow)\n",
    "        for fold in range(n_fold):\n",
    "            print('stacking target:', t, 'fold:', fold)\n",
    "            model = np.load(f'{base_path}/otto/otto_lgbm_fold{fold}_{t}.pkl', allow_pickle=True)\n",
    "            \n",
    "            df_pred_list = []\n",
    "            for i, v in enumerate(df_list):\n",
    "                tmp = model.predict(v)\n",
    "                df_pred_list.append(tmp)\n",
    "            df_pred += np.concatenate(df_pred_list)\n",
    "            \n",
    "        tmp_df = pd.DataFrame(df_pred/n_fold, columns=[f'pred_stack_{t}'], dtype='float64')\n",
    "        if i == 0:\n",
    "            res_df = tmp_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df, tmp_df], axis=1)\n",
    "            \n",
    "    return pd.concat([df, res_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XBANiP7Wfvwt"
   },
   "outputs": [],
   "source": [
    "# remove low importance features\n",
    "def remove_features(df):\n",
    "    DROP_COL = ['session_type_mean']\n",
    "    \n",
    "    hour_name_list = ['0_3oclock', '4_7oclock', '8_11oclock', '12_15oclock', '16_19oclock', '20_23oclock']\n",
    "    week_name_list = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    for h in hour_name_list:\n",
    "        DROP_COL += [f'session_{h}_action_ratio_1week']\n",
    "    for w in week_name_list:\n",
    "        DROP_COL += [f'session_{w}_action_ratio']\n",
    "\n",
    "    return df.drop(DROP_COL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5102,
     "status": "ok",
     "timestamp": 1674477428581,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = fe.reduce_memory(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10653,
     "status": "ok",
     "timestamp": 1674477439212,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eUrB2zTWf4ZK",
    "outputId": "26a3ff7f-f3fd-4845-d868-18a0146fc11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sum: 224413\n",
      "target mean: 0.009848794650219957\n"
     ]
    }
   ],
   "source": [
    "train = remove_negative_session(train, target)\n",
    "print('target sum:', train[target].sum())\n",
    "print('target mean:', train[target].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = fe.join_session_features(train, session_path)\n",
    "#train = fe.join_aid_features(train, aid_path)\n",
    "#train = fe.join_interactive_features(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Hyperparameter Tuning by Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    session = train['session']\n",
    "    unique_session = session.unique()\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [20],\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    N_splits = 5\n",
    "    kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}/{N_splits}....')\n",
    "        # session単位で分割してKFoldする\n",
    "        tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "        is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "        del tr_groups, va_groups\n",
    "        gc.collect()\n",
    "        # is_ir, is_va=Trueのindexを取得\n",
    "        trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "        del is_tr, is_va\n",
    "        gc.collect()\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        # under sampling\n",
    "        x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "        # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "        query_list_train = x_train['session'].value_counts()\n",
    "        query_list_train = query_list_train.sort_index()\n",
    "\n",
    "        query_list_valid = x_val['session'].value_counts()\n",
    "        query_list_valid = query_list_valid.sort_index()\n",
    "        \n",
    "\n",
    "        # memory節約のため, under sampling後にfeature追加\n",
    "        print('add session features....')\n",
    "        x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "        print('add aid features....')\n",
    "        x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "        print('add interactive features....')\n",
    "        x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "        print('remove features....')\n",
    "        x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "        print('remove id from features....')\n",
    "        x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 20,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439214,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "# feature170\n",
    "params = {'objective': 'lambdarank', \n",
    "          'metric': 'ndcg', \n",
    "          'ndcg_eval_at': [20], \n",
    "          'boosting': 'gbdt', \n",
    "          'seed': 42, \n",
    "          'n_jobs': -1, \n",
    "          #'learning_rate': 0.1, \n",
    "          'learning_rate': 0.05, \n",
    "          'feature_pre_filter': False, \n",
    "          'lambda_l1': 1.7510743847807332e-08, \n",
    "          'lambda_l2': 3.773149139134113e-07, \n",
    "          'num_leaves': 108, \n",
    "          'feature_fraction': 0.4, \n",
    "          'bagging_fraction': 1.0, \n",
    "          'bagging_freq': 0, \n",
    "          'min_child_samples': 20}\n",
    "\n",
    "# feature 210 -> not improved\n",
    "#params = {'objective': 'lambdarank', \n",
    "#          'metric': 'ndcg', \n",
    "#          'ndcg_eval_at': [20], \n",
    "#          'boosting': 'gbdt', \n",
    "#          'seed': 42, \n",
    "#          'n_jobs': -1, \n",
    "#          #'learning_rate': 0.1, \n",
    "#          'learning_rate': 0.05, \n",
    "#          'feature_pre_filter': False, \n",
    "#          'lambda_l1': 0.0, \n",
    "#          'lambda_l2': 0.0, \n",
    "#          'num_leaves': 95, \n",
    "#          'feature_fraction': 0.5, \n",
    "#          'bagging_fraction': 1.0, \n",
    "#          'bagging_freq': 0, \n",
    "#          'min_child_samples': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "9cc7f1c4-d87f-47b3-c476-5f23190d3cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0/5....\n",
      "before mean: 0.009829630132793749\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7165960, 214)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.306796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47354\n",
      "[LightGBM] [Info] Number of data points in the train set: 7165960, number of used features: 214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.880194\tvalid_1's ndcg@20: 0.814234\n",
      "[20]\ttraining's ndcg@20: 0.881891\tvalid_1's ndcg@20: 0.816458\n",
      "[30]\ttraining's ndcg@20: 0.883162\tvalid_1's ndcg@20: 0.817743\n",
      "[40]\ttraining's ndcg@20: 0.884327\tvalid_1's ndcg@20: 0.818438\n",
      "[50]\ttraining's ndcg@20: 0.885797\tvalid_1's ndcg@20: 0.819539\n",
      "[60]\ttraining's ndcg@20: 0.886944\tvalid_1's ndcg@20: 0.819904\n",
      "[70]\ttraining's ndcg@20: 0.888215\tvalid_1's ndcg@20: 0.820646\n",
      "[80]\ttraining's ndcg@20: 0.88945\tvalid_1's ndcg@20: 0.821411\n",
      "[90]\ttraining's ndcg@20: 0.890653\tvalid_1's ndcg@20: 0.822087\n",
      "[100]\ttraining's ndcg@20: 0.891896\tvalid_1's ndcg@20: 0.822376\n",
      "[110]\ttraining's ndcg@20: 0.89306\tvalid_1's ndcg@20: 0.822722\n",
      "[120]\ttraining's ndcg@20: 0.8941\tvalid_1's ndcg@20: 0.82293\n",
      "[130]\ttraining's ndcg@20: 0.89523\tvalid_1's ndcg@20: 0.823201\n",
      "[140]\ttraining's ndcg@20: 0.896266\tvalid_1's ndcg@20: 0.823423\n",
      "[150]\ttraining's ndcg@20: 0.897154\tvalid_1's ndcg@20: 0.823629\n",
      "[160]\ttraining's ndcg@20: 0.898049\tvalid_1's ndcg@20: 0.82394\n",
      "[170]\ttraining's ndcg@20: 0.898917\tvalid_1's ndcg@20: 0.824018\n",
      "[180]\ttraining's ndcg@20: 0.899805\tvalid_1's ndcg@20: 0.824147\n",
      "[190]\ttraining's ndcg@20: 0.900726\tvalid_1's ndcg@20: 0.824134\n",
      "[200]\ttraining's ndcg@20: 0.901569\tvalid_1's ndcg@20: 0.824345\n",
      "[210]\ttraining's ndcg@20: 0.902281\tvalid_1's ndcg@20: 0.824477\n",
      "[220]\ttraining's ndcg@20: 0.903001\tvalid_1's ndcg@20: 0.824501\n",
      "[230]\ttraining's ndcg@20: 0.903754\tvalid_1's ndcg@20: 0.824212\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's ndcg@20: 0.902919\tvalid_1's ndcg@20: 0.824561\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 0 orders recall = 0.7263676414472543\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1/5....\n",
      "before mean: 0.00985302291350443\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7188760, 214)\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.481468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47303\n",
      "[LightGBM] [Info] Number of data points in the train set: 7188760, number of used features: 214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.880685\tvalid_1's ndcg@20: 0.815314\n",
      "[20]\ttraining's ndcg@20: 0.882312\tvalid_1's ndcg@20: 0.817298\n",
      "[30]\ttraining's ndcg@20: 0.883764\tvalid_1's ndcg@20: 0.817962\n",
      "[40]\ttraining's ndcg@20: 0.884887\tvalid_1's ndcg@20: 0.818817\n",
      "[50]\ttraining's ndcg@20: 0.886174\tvalid_1's ndcg@20: 0.819446\n",
      "[60]\ttraining's ndcg@20: 0.887152\tvalid_1's ndcg@20: 0.820016\n",
      "[70]\ttraining's ndcg@20: 0.888406\tvalid_1's ndcg@20: 0.820735\n",
      "[80]\ttraining's ndcg@20: 0.889586\tvalid_1's ndcg@20: 0.821037\n",
      "[90]\ttraining's ndcg@20: 0.890681\tvalid_1's ndcg@20: 0.821509\n",
      "[100]\ttraining's ndcg@20: 0.891895\tvalid_1's ndcg@20: 0.821835\n",
      "[110]\ttraining's ndcg@20: 0.893089\tvalid_1's ndcg@20: 0.822435\n",
      "[120]\ttraining's ndcg@20: 0.894072\tvalid_1's ndcg@20: 0.822724\n",
      "[130]\ttraining's ndcg@20: 0.895102\tvalid_1's ndcg@20: 0.823219\n",
      "[140]\ttraining's ndcg@20: 0.896092\tvalid_1's ndcg@20: 0.823312\n",
      "[150]\ttraining's ndcg@20: 0.896979\tvalid_1's ndcg@20: 0.823411\n",
      "[160]\ttraining's ndcg@20: 0.897922\tvalid_1's ndcg@20: 0.823533\n",
      "[170]\ttraining's ndcg@20: 0.89882\tvalid_1's ndcg@20: 0.823407\n",
      "[180]\ttraining's ndcg@20: 0.899667\tvalid_1's ndcg@20: 0.823326\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's ndcg@20: 0.898317\tvalid_1's ndcg@20: 0.823653\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 1 orders recall = 0.7251614941721668\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2/5....\n",
      "before mean: 0.009842340724804607\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7175640, 214)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.579805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47346\n",
      "[LightGBM] [Info] Number of data points in the train set: 7175640, number of used features: 214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.880166\tvalid_1's ndcg@20: 0.815358\n",
      "[20]\ttraining's ndcg@20: 0.881773\tvalid_1's ndcg@20: 0.8171\n",
      "[30]\ttraining's ndcg@20: 0.883111\tvalid_1's ndcg@20: 0.81799\n",
      "[40]\ttraining's ndcg@20: 0.884181\tvalid_1's ndcg@20: 0.819195\n",
      "[50]\ttraining's ndcg@20: 0.885418\tvalid_1's ndcg@20: 0.820349\n",
      "[60]\ttraining's ndcg@20: 0.886474\tvalid_1's ndcg@20: 0.821175\n",
      "[70]\ttraining's ndcg@20: 0.887654\tvalid_1's ndcg@20: 0.821914\n",
      "[80]\ttraining's ndcg@20: 0.888937\tvalid_1's ndcg@20: 0.822529\n",
      "[90]\ttraining's ndcg@20: 0.890069\tvalid_1's ndcg@20: 0.823211\n",
      "[100]\ttraining's ndcg@20: 0.891189\tvalid_1's ndcg@20: 0.823461\n",
      "[110]\ttraining's ndcg@20: 0.892295\tvalid_1's ndcg@20: 0.823864\n",
      "[120]\ttraining's ndcg@20: 0.893345\tvalid_1's ndcg@20: 0.824125\n",
      "[130]\ttraining's ndcg@20: 0.894277\tvalid_1's ndcg@20: 0.824497\n",
      "[140]\ttraining's ndcg@20: 0.895304\tvalid_1's ndcg@20: 0.824657\n",
      "[150]\ttraining's ndcg@20: 0.896276\tvalid_1's ndcg@20: 0.824597\n",
      "[160]\ttraining's ndcg@20: 0.897266\tvalid_1's ndcg@20: 0.825322\n",
      "[170]\ttraining's ndcg@20: 0.898145\tvalid_1's ndcg@20: 0.825183\n",
      "[180]\ttraining's ndcg@20: 0.898963\tvalid_1's ndcg@20: 0.824996\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttraining's ndcg@20: 0.897379\tvalid_1's ndcg@20: 0.825351\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 2 orders recall = 0.7292363751029203\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3/5....\n",
      "before mean: 0.009842684972505789\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7174000, 214)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.262183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47315\n",
      "[LightGBM] [Info] Number of data points in the train set: 7174000, number of used features: 214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.879822\tvalid_1's ndcg@20: 0.815412\n",
      "[20]\ttraining's ndcg@20: 0.881558\tvalid_1's ndcg@20: 0.81703\n",
      "[30]\ttraining's ndcg@20: 0.882853\tvalid_1's ndcg@20: 0.818117\n",
      "[40]\ttraining's ndcg@20: 0.884005\tvalid_1's ndcg@20: 0.819302\n",
      "[50]\ttraining's ndcg@20: 0.885417\tvalid_1's ndcg@20: 0.820467\n",
      "[60]\ttraining's ndcg@20: 0.886558\tvalid_1's ndcg@20: 0.821323\n",
      "[70]\ttraining's ndcg@20: 0.887793\tvalid_1's ndcg@20: 0.821935\n",
      "[80]\ttraining's ndcg@20: 0.888949\tvalid_1's ndcg@20: 0.822543\n",
      "[90]\ttraining's ndcg@20: 0.890178\tvalid_1's ndcg@20: 0.822894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's ndcg@20: 0.891274\tvalid_1's ndcg@20: 0.823432\n",
      "[110]\ttraining's ndcg@20: 0.892242\tvalid_1's ndcg@20: 0.82366\n",
      "[120]\ttraining's ndcg@20: 0.89339\tvalid_1's ndcg@20: 0.824012\n",
      "[130]\ttraining's ndcg@20: 0.894371\tvalid_1's ndcg@20: 0.824328\n",
      "[140]\ttraining's ndcg@20: 0.895275\tvalid_1's ndcg@20: 0.824709\n",
      "[150]\ttraining's ndcg@20: 0.896122\tvalid_1's ndcg@20: 0.824818\n",
      "[160]\ttraining's ndcg@20: 0.89702\tvalid_1's ndcg@20: 0.824927\n",
      "[170]\ttraining's ndcg@20: 0.89791\tvalid_1's ndcg@20: 0.824974\n",
      "[180]\ttraining's ndcg@20: 0.898683\tvalid_1's ndcg@20: 0.824961\n",
      "[190]\ttraining's ndcg@20: 0.899542\tvalid_1's ndcg@20: 0.825077\n",
      "[200]\ttraining's ndcg@20: 0.900349\tvalid_1's ndcg@20: 0.825134\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's ndcg@20: 0.899212\tvalid_1's ndcg@20: 0.825266\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 3 orders recall = 0.7187564623974633\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4/5....\n",
      "before mean: 0.009876283562224624\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7201720, 214)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.258234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47326\n",
      "[LightGBM] [Info] Number of data points in the train set: 7201720, number of used features: 214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.88006\tvalid_1's ndcg@20: 0.815538\n",
      "[20]\ttraining's ndcg@20: 0.881594\tvalid_1's ndcg@20: 0.817503\n",
      "[30]\ttraining's ndcg@20: 0.882985\tvalid_1's ndcg@20: 0.818325\n",
      "[40]\ttraining's ndcg@20: 0.884174\tvalid_1's ndcg@20: 0.819336\n",
      "[50]\ttraining's ndcg@20: 0.88543\tvalid_1's ndcg@20: 0.820074\n",
      "[60]\ttraining's ndcg@20: 0.88675\tvalid_1's ndcg@20: 0.820755\n",
      "[70]\ttraining's ndcg@20: 0.887966\tvalid_1's ndcg@20: 0.821471\n",
      "[80]\ttraining's ndcg@20: 0.889053\tvalid_1's ndcg@20: 0.821993\n",
      "[90]\ttraining's ndcg@20: 0.890196\tvalid_1's ndcg@20: 0.822425\n",
      "[100]\ttraining's ndcg@20: 0.891341\tvalid_1's ndcg@20: 0.823064\n",
      "[110]\ttraining's ndcg@20: 0.892314\tvalid_1's ndcg@20: 0.823352\n",
      "[120]\ttraining's ndcg@20: 0.893434\tvalid_1's ndcg@20: 0.823642\n",
      "[130]\ttraining's ndcg@20: 0.894531\tvalid_1's ndcg@20: 0.823923\n",
      "[140]\ttraining's ndcg@20: 0.895513\tvalid_1's ndcg@20: 0.823873\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's ndcg@20: 0.894144\tvalid_1's ndcg@20: 0.823964\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 4 orders recall = 0.7270135297867819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "session = train['session']\n",
    "unique_session = session.unique()\n",
    "\n",
    "N_splits = 5\n",
    "kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}/{N_splits}....')\n",
    "    # session単位で分割してKFoldする\n",
    "    tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "    is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "    del tr_groups, va_groups\n",
    "    gc.collect()\n",
    "    # is_ir, is_va=Trueのindexを取得\n",
    "    trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "    del is_tr, is_va\n",
    "    gc.collect()\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "    query_list_train = x_train['session'].value_counts()\n",
    "    query_list_train = query_list_train.sort_index()\n",
    "\n",
    "    query_list_valid = x_val['session'].value_counts()\n",
    "    query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "    # memory節約のため, under sampling後にfeature追加\n",
    "    print('add session features....')\n",
    "    x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "    print('add aid features....')\n",
    "    x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "    print('add interactive features....')\n",
    "    x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "    print('remove features....')\n",
    "    x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "    print('remove id from features....')\n",
    "    x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        print('cross_feature stacking, x_train...')\n",
    "        x_train = add_cross_stacking_feature(x_train, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        print('cross_feature stacking, x_val...')\n",
    "        x_val = add_cross_stacking_feature(x_val, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "    del x_train, y_train, query_list_train, query_list_valid\n",
    "    gc.collect()\n",
    "\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        #num_boost_round = 100,\n",
    "        num_boost_round = 2000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 20,\n",
    "        verbose_eval = 10,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "    # Save best model\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}_stack.pkl')\n",
    "    else:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    # tmp recall for each fold\n",
    "    df = pd.DataFrame(val_pred, columns=[\"score\"])\n",
    "    tmp = train[['session', 'aid']].iloc[val_ind].reset_index(drop=True)\n",
    "    pred_df = pd.concat([tmp, df], axis=1)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "    pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "    pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "    pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "    pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "    pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "    pred_df = pred_df.drop(['aid'],axis=1)\n",
    "\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "    test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "    # foldごとのreallなのでinnter\n",
    "    test_labels = test_labels.merge(sub, how='inner', on=['session']) \n",
    "    test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    print(f'fold {fold} {TYPE_MODE} recall =',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_orders</td>\n",
       "      <td>11830 1732105 588923 876129 884502 1157882 307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098530_orders</td>\n",
       "      <td>409236 1603001 264500 364155 963957 254154 210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098531_orders</td>\n",
       "      <td>1365569 1728212 1271998 1557766 1309633 452188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098533_orders</td>\n",
       "      <td>1074173 1309900 1165015 765030 833149 1622419 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098534_orders</td>\n",
       "      <td>223062 1607945 1342293 908024 1449202 1649004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133618</th>\n",
       "      <td>12899159_orders</td>\n",
       "      <td>1512596 1383649 1131172 314450 1616589 1843582...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133619</th>\n",
       "      <td>12899329_orders</td>\n",
       "      <td>1333457 1470364 356732 1667554 977011 1376476 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133620</th>\n",
       "      <td>12899337_orders</td>\n",
       "      <td>558573 1662401 742581 1223508 1581401 554660 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133621</th>\n",
       "      <td>12899373_orders</td>\n",
       "      <td>1766353 487949 461938 516917 995962 1662986 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133622</th>\n",
       "      <td>12899525_orders</td>\n",
       "      <td>996393 1599360 127479 1488793 1123744 405179 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133623 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           session_type                                             labels\n",
       "0       11098528_orders  11830 1732105 588923 876129 884502 1157882 307...\n",
       "1       11098530_orders  409236 1603001 264500 364155 963957 254154 210...\n",
       "2       11098531_orders  1365569 1728212 1271998 1557766 1309633 452188...\n",
       "3       11098533_orders  1074173 1309900 1165015 765030 833149 1622419 ...\n",
       "4       11098534_orders  223062 1607945 1342293 908024 1449202 1649004 ...\n",
       "...                 ...                                                ...\n",
       "133618  12899159_orders  1512596 1383649 1131172 314450 1616589 1843582...\n",
       "133619  12899329_orders  1333457 1470364 356732 1667554 977011 1376476 ...\n",
       "133620  12899337_orders  558573 1662401 742581 1223508 1581401 554660 8...\n",
       "133621  12899373_orders  1766353 487949 461938 516917 995962 1662986 18...\n",
       "133622  12899525_orders  996393 1599360 127479 1488793 1123744 405179 3...\n",
       "\n",
       "[133623 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "if CROSS_TARGET_STACKING:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}_stack.parquet')\n",
    "else:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}.parquet')\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders recall = 0.66254392712486\n"
     ]
    }
   ],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# ranker model, fold0を factor=1.111369 で割ればそれっぽい値が出る, each foldは session inner joinなので高めに出る\n",
    "# num=100, lr=0.1, no feature add, valid_1's ndcg@50: 0.843586, fold 0 orders recall = 0.7263385039885385, orders recall = 0.6535526311589739\n",
    "# add aid feature, valid_1's ndcg@50: 0.84962, fold 0 orders recall = 0.7305304490864389, (orders recall = 0.657324?)\n",
    "# add aid + session feature, valid_1's ndcg@50: 0.849762, fold 0 orders recall = 0.7302651361055592, orders recall = 0.6575806806829172\n",
    "# all valid_1's ndcg@50: 0.848664, fold 0 orders recall = 0.730990324919964, orders recall = 0.6579892308723504\n",
    "\n",
    "\n",
    "# hypter paramやりなおし、regularization param大幅変更 num=100, lr=0.1\n",
    "# no add: valid_1's ndcg@20: 0.835401, fold 0 orders recall = 0.7261793162000106, orders recall = 0.6535877409408783  -> order,carts差し替えPB = 0.581\n",
    "#                                                                                 carts recall = 0.41837386076234817\n",
    "# sessionのみ: valid_1's ndcg@20: 0.836164, fold 0 orders recall = 0.7268514424182394, orders recall = 0.6545069788671031 -> order,carts差し替えPB = 0.583\n",
    "#                                                                                      carts recall = 0.4196106730132077\n",
    "# aidのみ: valid_1's ndcg@20: 0.842205, fold 0 orders recall = 0.7302828236376179, orders recall = 0.6575838724812721 -> order,carts差し替えPB = 0.586 (55, 1/19)\n",
    "#                                                                                  carts recall = 0.42261163401459195                              \n",
    "# aid+session: valid_1's ndcg@20: 0.843169, fold 0 orders recall = 0.7309018872596706, orders recall = 0.6582030813621319 -> order,carts差し替えPB = 0.587 (54, 1/19)\n",
    "#                                                                                      carts recall = 0.42347896378377814\n",
    "# all: valid_1's ndcg@20: 0.843514, fold 0 orders recall = 0.731184887772609, orders recall = 0.6584137400535583 -> order,carts差し替えPB = 0.586 下がったけどブレ？\n",
    "#                                                                             carts recall = 0.42349804503870025\n",
    "# num=1000, lr=0.05 [281] valid_1's ndcg@20: 0.844737, fold 0 orders recall = 0.7315386384137821, orders recall = 0.6586627003252442 -> PB = 0.587 (53, 1/19)\n",
    "#                                                                                                 carts recall = 0.4242057861303562\n",
    "#                                                                                                 clicks recall = 0.536971\n",
    "# candidate add, mean 60 -> 120\n",
    "# lr=0.1, [100] valid_1's ndcg@20: 0.832289 fold 0 orders recall = 0.7260696029689797, orders recall = 0.6619853624127442-> PB = 0.592 \n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.833279,fold 0 orders recall = 0.7268398571528605, orders recall = 0.6622981586515291\n",
    "#                                                                                      carts recall = 0.4296682290166909\n",
    "#                                                                                      clicks recall = 0.54153778850196010\n",
    "# candidate add more, 120->140\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.824422, fold 0 orders recall = 0.7264196759981961, orders recall = 0.6627609694129963-> PB = 0.592\n",
    "#                                                                                       carts recall = 0.43018862687820264\n",
    "#                                                                                       clicks recall = 0.5425910292822583\n",
    "# stacking test, be careful to leak..\n",
    "# lr=0.1,  valid_1's ndcg@20: 0.825827 [100] fold 0 orders recall = 0.7276858500711139, orders recall = 0.6645771026769612\n",
    "# lr=0.05 valid_1's ndcg@20: 0.826031 [131] fold 0 orders recall = 0.728258230131474, orders recall = 0.6647973367634527\n",
    "#                                                                                    carts recall = 0.432039508605646\n",
    "# add week/date feature\n",
    "# lr=0.1, candidate 120 ver[100] valid_1's ndcg@20: 0.832938, fold 0 orders recall = 0.7262971780687627,  orders recall = 0.6620491983798431\n",
    "# add cos sim, feature214 [100], valid_1's ndcg@20: 0.832677, fold 0 orders recall = 0.7267348224914222, orders recall = 0.6623843372071127\n",
    "# remove 1week session hour/week, valid_1's ndcg@20: 0.833548, fold 0 orders recall = 0.7265597647223584, orders recall = 0.6620843081617476\n",
    "\n",
    "# lr=0.05+candidate140, orders recall = 0.6627354350261568\n",
    "# Optuna valid_1's ndcg@20: 0.824561 [219] fold 0 orders recall = 0.7263676414472543, orders recall = 0.66254392712486\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
