{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7228,
     "status": "ok",
     "timestamp": 1674477402817,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e",
    "outputId": "9e930ba3-d13a-4ee8-f28a-a681a3840d33"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    notebook_path = base_path + '/otto/notebook'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'\n",
    "    notebook_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1674477405015,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "sys.path.append(f\"{notebook_path}/../src/\")\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18480,
     "status": "ok",
     "timestamp": 1674477423492,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_0.parquet') # 0.592\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_0_ver2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "OPTUNA_FLAG = False\n",
    "CROSS_TARGET_STACKING = True\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL_ID = ['session','aid']\n",
    "\n",
    "#TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "TYPE_MODE = 'orders'\n",
    "IGNORE_COL_TARGET = ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    #pos_neg_ratio = 1/39\n",
    "    pos_neg_ratio = 1/29 # 3.3%\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'clicks']\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['orders', 'clicks']\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "    # used for cross target stacking\n",
    "    cross_target_list = ['carts', 'clicks']\n",
    "\n",
    "session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df, _target):\n",
    "    true_df = df.groupby('session')[_target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio, random_state=0).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "    # sessionの順番がばらばらになるので再びsort\n",
    "    tmp = pd.concat([after_x, after_y], axis=1).sort_values('session')\n",
    "    after_y = tmp[target]\n",
    "    after_x = tmp.drop(target , axis=1)\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y\n",
    "\n",
    "# dataframe, target_list, number of split data, number of fold\n",
    "def add_cross_stacking_feature(df, cross_target_list, n_div, n_fold, base_path):\n",
    "    # split data in order to save memory\n",
    "    Nrow = df.shape[0]\n",
    "    n = int(Nrow // n_div) + 1\n",
    "    df_list = []\n",
    "    for i in range(n_div):\n",
    "        tmp = df.iloc[i*n : (i+1)*n, :]\n",
    "        df_list.append(tmp)\n",
    "    \n",
    "    # initialization\n",
    "    res_df = pd.DataFrame(columns=[], index = [])\n",
    "    for i, t in enumerate(cross_target_list):\n",
    "        df_pred = np.zeros(Nrow)\n",
    "        for fold in range(n_fold):\n",
    "            print('stacking target:', t, 'fold:', fold)\n",
    "            model = np.load(f'{base_path}/otto/otto_lgbm_fold{fold}_{t}.pkl', allow_pickle=True)\n",
    "            \n",
    "            df_pred_list = []\n",
    "            for i, v in enumerate(df_list):\n",
    "                tmp = model.predict(v)\n",
    "                df_pred_list.append(tmp)\n",
    "            df_pred += np.concatenate(df_pred_list)\n",
    "            \n",
    "        tmp_df = pd.DataFrame(df_pred/n_fold, columns=[f'pred_stack_{t}'], dtype='float64')\n",
    "        if i == 0:\n",
    "            res_df = tmp_df\n",
    "        else:\n",
    "            res_df = pd.concat([res_df, tmp_df], axis=1)\n",
    "            \n",
    "    return pd.concat([df, res_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XBANiP7Wfvwt"
   },
   "outputs": [],
   "source": [
    "# importanceが極端に低いものを削る (18件)\n",
    "def remove_features(df):\n",
    "    DROP_COL = ['session_type_mean']\n",
    "    return df.drop(DROP_COL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5102,
     "status": "ok",
     "timestamp": 1674477428581,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = fe.reduce_memory(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10653,
     "status": "ok",
     "timestamp": 1674477439212,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eUrB2zTWf4ZK",
    "outputId": "26a3ff7f-f3fd-4845-d868-18a0146fc11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sum: 224413\n",
      "target mean: 0.009848794650219957\n"
     ]
    }
   ],
   "source": [
    "train = remove_negative_session(train, target)\n",
    "print('target sum:', train[target].sum())\n",
    "print('target mean:', train[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Hyperparameter Tuning by Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    session = train['session']\n",
    "    unique_session = session.unique()\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [20],\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    N_splits = 5\n",
    "    kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}/{N_splits}....')\n",
    "        # session単位で分割してKFoldする\n",
    "        tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "        is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "        del tr_groups, va_groups\n",
    "        gc.collect()\n",
    "        # is_ir, is_va=Trueのindexを取得\n",
    "        trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "        del is_tr, is_va\n",
    "        gc.collect()\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        # under sampling\n",
    "        x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "        # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "        query_list_train = x_train['session'].value_counts()\n",
    "        query_list_train = query_list_train.sort_index()\n",
    "\n",
    "        query_list_valid = x_val['session'].value_counts()\n",
    "        query_list_valid = query_list_valid.sort_index()\n",
    "        \n",
    "\n",
    "        # memory節約のため, under sampling後にfeature追加\n",
    "        print('add session features....')\n",
    "        x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "        print('add aid features....')\n",
    "        x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "        print('add interactive features....')\n",
    "        x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "        print('remove features....')\n",
    "        x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "        print('remove id from features....')\n",
    "        x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 20,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439214,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "params = {'objective': 'lambdarank', \n",
    "          'metric': 'ndcg', \n",
    "          'ndcg_eval_at': [20], \n",
    "          'boosting': 'gbdt', \n",
    "          'seed': 42, \n",
    "          'n_jobs': -1, \n",
    "          'learning_rate': 0.1, \n",
    "          #'learning_rate': 0.05, \n",
    "          'feature_pre_filter': False, \n",
    "          'lambda_l1': 1.7510743847807332e-08, \n",
    "          'lambda_l2': 3.773149139134113e-07, \n",
    "          'num_leaves': 108, \n",
    "          'feature_fraction': 0.4, \n",
    "          'bagging_fraction': 1.0, \n",
    "          'bagging_freq': 0, \n",
    "          'min_child_samples': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "9cc7f1c4-d87f-47b3-c476-5f23190d3cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0/5....\n",
      "before mean: 0.009829630132793749\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7165960, 173)\n",
      "cross_feature stacking... carts clicks\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "stacking target: carts fold: 0\n",
      "stacking target: carts fold: 1\n",
      "stacking target: carts fold: 2\n",
      "stacking target: carts fold: 3\n",
      "stacking target: carts fold: 4\n",
      "stacking target: clicks fold: 0\n",
      "stacking target: clicks fold: 1\n",
      "stacking target: clicks fold: 2\n",
      "stacking target: clicks fold: 3\n",
      "stacking target: clicks fold: 4\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.454015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 37409\n",
      "[LightGBM] [Info] Number of data points in the train set: 7165960, number of used features: 175\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.885888\tvalid_1's ndcg@20: 0.820871\n",
      "[20]\ttraining's ndcg@20: 0.888135\tvalid_1's ndcg@20: 0.822016\n",
      "[30]\ttraining's ndcg@20: 0.890324\tvalid_1's ndcg@20: 0.823537\n",
      "[40]\ttraining's ndcg@20: 0.892734\tvalid_1's ndcg@20: 0.824533\n",
      "[50]\ttraining's ndcg@20: 0.895087\tvalid_1's ndcg@20: 0.825026\n",
      "[60]\ttraining's ndcg@20: 0.897242\tvalid_1's ndcg@20: 0.825329\n",
      "[70]\ttraining's ndcg@20: 0.899316\tvalid_1's ndcg@20: 0.825593\n",
      "[80]\ttraining's ndcg@20: 0.901159\tvalid_1's ndcg@20: 0.825448\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's ndcg@20: 0.898408\tvalid_1's ndcg@20: 0.825827\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 0 orders recall = 0.7276858500711139\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1/5....\n",
      "before mean: 0.00985302291350443\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (7188760, 173)\n",
      "cross_feature stacking... carts clicks\n",
      "stacking target: carts fold: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "session = train['session']\n",
    "unique_session = session.unique()\n",
    "\n",
    "N_splits = 5\n",
    "kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}/{N_splits}....')\n",
    "    # session単位で分割してKFoldする\n",
    "    tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "    is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "    del tr_groups, va_groups\n",
    "    gc.collect()\n",
    "    # is_ir, is_va=Trueのindexを取得\n",
    "    trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "    del is_tr, is_va\n",
    "    gc.collect()\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "    query_list_train = x_train['session'].value_counts()\n",
    "    query_list_train = query_list_train.sort_index()\n",
    "\n",
    "    query_list_valid = x_val['session'].value_counts()\n",
    "    query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "    # memory節約のため, under sampling後にfeature追加\n",
    "    print('add session features....')\n",
    "    x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "    print('add aid features....')\n",
    "    x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "    print('add interactive features....')\n",
    "    x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "    print('remove features....')\n",
    "    x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "    print('remove id from features....')\n",
    "    x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        print('cross_feature stacking, x_train...')\n",
    "        x_train = add_cross_stacking_feature(x_train, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        print('cross_feature stacking, x_val...')\n",
    "        x_val = add_cross_stacking_feature(x_val, cross_target_list, n_div=5, n_fold=5, base_path=base_path)\n",
    "        \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 100,\n",
    "        #num_boost_round = 2000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 20,\n",
    "        verbose_eval = 10,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "    # Save best model\n",
    "    \n",
    "    if CROSS_TARGET_STACKING:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}_stack.pkl')\n",
    "    else:\n",
    "        joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    # tmp recall for each fold\n",
    "    df = pd.DataFrame(val_pred, columns=[\"score\"])\n",
    "    tmp = train[['session', 'aid']].iloc[val_ind].reset_index(drop=True)\n",
    "    pred_df = pd.concat([tmp, df], axis=1)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "    pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "    pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "    pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "    pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "    pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "    pred_df = pred_df.drop(['aid'],axis=1)\n",
    "\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "    test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "    # foldごとのreallなのでinnter\n",
    "    test_labels = test_labels.merge(sub, how='inner', on=['session']) \n",
    "    test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    print(f'fold {fold} {TYPE_MODE} recall =',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "if CROSS_TARGET_STACKING:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}_stack.parquet')\n",
    "else:\n",
    "    pred_df.to_parquet(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}.parquet')\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# ranker model, fold0を factor=1.111369 で割ればそれっぽい値が出る, each foldは session inner joinなので高めに出る\n",
    "# num=100, lr=0.1, no feature add, valid_1's ndcg@50: 0.843586, fold 0 orders recall = 0.7263385039885385, orders recall = 0.6535526311589739\n",
    "# add aid feature, valid_1's ndcg@50: 0.84962, fold 0 orders recall = 0.7305304490864389, (orders recall = 0.657324?)\n",
    "# add aid + session feature, valid_1's ndcg@50: 0.849762, fold 0 orders recall = 0.7302651361055592, orders recall = 0.6575806806829172\n",
    "# all valid_1's ndcg@50: 0.848664, fold 0 orders recall = 0.730990324919964, orders recall = 0.6579892308723504\n",
    "\n",
    "\n",
    "# hypter paramやりなおし、regularization param大幅変更 num=100, lr=0.1\n",
    "# no add: valid_1's ndcg@20: 0.835401, fold 0 orders recall = 0.7261793162000106, orders recall = 0.6535877409408783  -> order,carts差し替えPB = 0.581\n",
    "#                                                                                 carts recall = 0.41837386076234817\n",
    "# sessionのみ: valid_1's ndcg@20: 0.836164, fold 0 orders recall = 0.7268514424182394, orders recall = 0.6545069788671031 -> order,carts差し替えPB = 0.583\n",
    "#                                                                                      carts recall = 0.4196106730132077\n",
    "# aidのみ: valid_1's ndcg@20: 0.842205, fold 0 orders recall = 0.7302828236376179, orders recall = 0.6575838724812721 -> order,carts差し替えPB = 0.586 (55, 1/19)\n",
    "#                                                                                  carts recall = 0.42261163401459195                              \n",
    "# aid+session: valid_1's ndcg@20: 0.843169, fold 0 orders recall = 0.7309018872596706, orders recall = 0.6582030813621319 -> order,carts差し替えPB = 0.587 (54, 1/19)\n",
    "#                                                                                      carts recall = 0.42347896378377814\n",
    "# all: valid_1's ndcg@20: 0.843514, fold 0 orders recall = 0.731184887772609, orders recall = 0.6584137400535583 -> order,carts差し替えPB = 0.586 下がったけどブレ？\n",
    "#                                                                             carts recall = 0.42349804503870025\n",
    "# num=1000, lr=0.05 [281] valid_1's ndcg@20: 0.844737, fold 0 orders recall = 0.7315386384137821, orders recall = 0.6586627003252442 -> PB = 0.587 (53, 1/19)\n",
    "#                                                                                                 carts recall = 0.4242057861303562\n",
    "#                                                                                                 clicks recall = 0.536971\n",
    "# candidate add, mean 60 -> 120\n",
    "# lr=0.1, [100] valid_1's ndcg@20: 0.832289 fold 0 orders recall = 0.7260696029689797, orders recall = 0.6619853624127442\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.833279,fold 0 orders recall = 0.7268398571528605, orders recall = 0.6622981586515291\n",
    "#                                                                                      carts recall = 0.4296682290166909\n",
    "#                                                                                      clicks recall = 0.54153778850196010\n",
    "# candidate add more\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.824422, fold 0 orders recall = 0.7264196759981961, orders recall = 0.6627609694129963\n",
    "#                                                                                       carts recall = 0.43018862687820264\n",
    "#                                                                                       clicks recall = 0.5425910292822583\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
