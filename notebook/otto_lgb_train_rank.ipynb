{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7228,
     "status": "ok",
     "timestamp": 1674477402817,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "8QrdrLFrx86e",
    "outputId": "9e930ba3-d13a-4ee8-f28a-a681a3840d33"
   },
   "outputs": [],
   "source": [
    "# True: Google Colab Notebook\n",
    "# False: My local PC\n",
    "colab = False\n",
    "if colab: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/output/otto/\n",
    "    base_path = '/content/drive/MyDrive'\n",
    "    notebook_path = base_path + '/otto/notebook'\n",
    "    !pip3 install optuna\n",
    "else:\n",
    "    base_path = '../data'\n",
    "    notebook_path = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rApCp4mVyLAk"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1674477405015,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "S8Rxu2iww5-9"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import sys\n",
    "sys.path.append(f\"{notebook_path}/../src/\")\n",
    "import feature_engineering as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18480,
     "status": "ok",
     "timestamp": 1674477423492,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "oujqBvdabvAs"
   },
   "outputs": [],
   "source": [
    "#train = pd.read_parquet('/content/drive/MyDrive/output/otto/train_50.parquet')\n",
    "#train = pd.read_parquet(f'{base_path}/output/otto/train_50_tmp.parquet')\n",
    "train = pd.read_parquet(f'{base_path}/output/otto/train_50_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "nE1xweGKyW0a"
   },
   "outputs": [],
   "source": [
    "DEBUG_MODE = False\n",
    "#DEBUG_MODE = True\n",
    "\n",
    "OPTUNA_FLAG = False\n",
    "#OPTUNA_FLAG = True\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    train = train.head(100000)\n",
    "IGNORE_COL_ID = ['session','aid']\n",
    "\n",
    "TYPE_MODE = 'clicks'\n",
    "#TYPE_MODE = 'carts'\n",
    "#TYPE_MODE = 'orders'\n",
    "IGNORE_COL_TARGET = ['y_clicks', 'y_carts', 'y_orders']\n",
    "\n",
    "\n",
    "if TYPE_MODE == 'clicks':\n",
    "    target = 'y_clicks'\n",
    "    # under sampling 1.3 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "elif TYPE_MODE == 'carts':\n",
    "    target = 'y_carts'\n",
    "    # under sampling 1.6 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "elif TYPE_MODE == 'orders':\n",
    "    target = 'y_orders'\n",
    "    # under sampling 2.1 -> 2.5%\n",
    "    pos_neg_ratio = 1/39\n",
    "\n",
    "session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "ZHnc3hihwSHY"
   },
   "outputs": [],
   "source": [
    "# 負例しかないものは学習に使えないので削る（学習のみ）\n",
    "def remove_negative_session(df):\n",
    "    true_df = df.groupby('session')[target].agg('sum') > 0\n",
    "    session = pd.DataFrame(true_df[true_df]).reset_index()['session']\n",
    "    df = df.merge(session, how = 'inner', on = 'session')\n",
    "    return df\n",
    "\n",
    "# 負例が多すぎる場合にunder samplingする\n",
    "# ratio = pos/neg\n",
    "def negative_sampling(df_x, df_y, ratio):\n",
    "    print('before mean:', df_y.mean())\n",
    "\n",
    "    Nrow = df_x.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "\n",
    "    df_x_list = [df_x.iloc[i*n : (i+1)*n, :] for i in range(Ndiv)]\n",
    "    df_y_list = [df_y.iloc[i*n : (i+1)*n] for i in range(Ndiv)]\n",
    "    del df_x, df_y\n",
    "    gc.collect()\n",
    "\n",
    "    for i in range(Ndiv):\n",
    "        print('under sampling.......',i + 1 , '/', Ndiv)\n",
    "        tmpx, tmpy = RandomUnderSampler(sampling_strategy=ratio, random_state=0).fit_resample(df_x_list[i], df_y_list[i])\n",
    "        df_x_list[i] = tmpx\n",
    "        df_y_list[i] = tmpy\n",
    "        del tmpx, tmpy\n",
    "        gc.collect()\n",
    "    print('under sampling end')\n",
    "    after_x = pd.concat(df_x_list)\n",
    "    del df_x_list\n",
    "    gc.collect()\n",
    "    print('post proccess1')\n",
    "    after_y = pd.concat(df_y_list)\n",
    "    del df_y_list\n",
    "    gc.collect()\n",
    "    # sessionの順番がばらばらになるので再びsort\n",
    "    tmp = pd.concat([after_x, after_y], axis=1).sort_values('session')\n",
    "    after_y = tmp[target]\n",
    "    after_x = tmp.drop(target , axis=1)\n",
    "\n",
    "    print('after mean:', after_y.mean())\n",
    "    return after_x, after_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1674477423493,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "XBANiP7Wfvwt"
   },
   "outputs": [],
   "source": [
    "# importanceが極端に低いものを削る (18件)\n",
    "def remove_features(df):\n",
    "    DROP_COL = ['session_type_mean']\n",
    "    return df.drop(DROP_COL, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5102,
     "status": "ok",
     "timestamp": 1674477428581,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "fr7uXIlVxvEJ"
   },
   "outputs": [],
   "source": [
    "train = fe.reduce_memory(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10653,
     "status": "ok",
     "timestamp": 1674477439212,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "eUrB2zTWf4ZK",
    "outputId": "26a3ff7f-f3fd-4845-d868-18a0146fc11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target sum: 1073848\n",
      "target mean: 0.00813328163752284\n"
     ]
    }
   ],
   "source": [
    "train = remove_negative_session(train)\n",
    "print('target sum:', train[target].sum())\n",
    "print('target mean:', train[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOW-eeZAyZT8"
   },
   "source": [
    "# Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "we5IplsR8tQ2"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import itertools\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# optuna\n",
    "if OPTUNA_FLAG:\n",
    "    import optuna.integration.lightgbm as lgb\n",
    "else:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439213,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "yjzMNkv_MGj9"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    session = train['session']\n",
    "    unique_session = session.unique()\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'ndcg_eval_at': [20],\n",
    "        'boosting': 'gbdt',\n",
    "        'seed': 42,        \n",
    "        'n_jobs': -1,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    N_splits = 5\n",
    "    kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "    for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold}/{N_splits}....')\n",
    "        # session単位で分割してKFoldする\n",
    "        tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "        is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "        del tr_groups, va_groups\n",
    "        gc.collect()\n",
    "        # is_ir, is_va=Trueのindexを取得\n",
    "        trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "        del is_tr, is_va\n",
    "        gc.collect()\n",
    "\n",
    "        y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "        train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "        x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "        del train_tmp\n",
    "        gc.collect()\n",
    "\n",
    "        # under sampling\n",
    "        x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "        # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "        query_list_train = x_train['session'].value_counts()\n",
    "        #x_train = x_train.drop('session' , axis=1)\n",
    "        query_list_train = query_list_train.sort_index()\n",
    "\n",
    "        query_list_valid = x_val['session'].value_counts()\n",
    "        #x_val = x_val.drop('session' , axis=1)\n",
    "        query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "        # memory節約のため, under sampling後にfeature追加\n",
    "        print('add session features....')\n",
    "        #x_train, x_val = fe.join_session_features(x_train), join_session_features(x_val)\n",
    "        print('add aid features....')\n",
    "        #x_train, x_val = fe.join_aid_features(x_train), join_aid_features(x_val)\n",
    "        print('add interactive features....')\n",
    "        #x_train, x_val = fe.join_interactive_features(x_train), join_interactive_features(x_val)\n",
    "        print('remove features....')\n",
    "        #x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "        print('remove id from features....')\n",
    "        x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "        print('x_train shape:', x_train.shape)\n",
    "\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "        del x_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "        #lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            #num_boost_round = 10500,\n",
    "            num_boost_round = 100,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 20,\n",
    "            verbose_eval = 10,\n",
    "            )\n",
    "        del lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "        break\n",
    "    model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1674477439214,
     "user": {
      "displayName": "テッツォ",
      "userId": "07789339878611604425"
     },
     "user_tz": -540
    },
    "id": "aTlbPCG4PKNS"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_FLAG:\n",
    "    print(\"Optuna results: \",model.params)\n",
    "\n",
    "params = {'objective': 'lambdarank',\n",
    "          'metric': 'ndcg',\n",
    "          'map_eval_at': [20],\n",
    "          'boosting': 'gbdt',\n",
    "          'seed': 42,\n",
    "          'n_jobs': -1,\n",
    "          #'learning_rate': 0.05,\n",
    "          'learning_rate': 0.1,\n",
    "          'feature_pre_filter': False,\n",
    "          'lambda_l1': 0.0, \n",
    "          'lambda_l2': 0.0, \n",
    "          'num_leaves': 222, \n",
    "          'feature_fraction': 0.62, \n",
    "          'bagging_fraction': 1.0, \n",
    "          'bagging_freq': 0, \n",
    "          'min_child_samples': 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6kHtxYa93k_",
    "outputId": "9cc7f1c4-d87f-47b3-c476-5f23190d3cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0/5....\n",
      "before mean: 0.008132127212028852\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (34363120, 168)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.626337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36181\n",
      "[LightGBM] [Info] Number of data points in the train set: 34363120, number of used features: 168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.747103\tvalid_1's ndcg@20: 0.605784\n",
      "[20]\ttraining's ndcg@20: 0.748789\tvalid_1's ndcg@20: 0.60746\n",
      "[30]\ttraining's ndcg@20: 0.750063\tvalid_1's ndcg@20: 0.608366\n",
      "[40]\ttraining's ndcg@20: 0.751233\tvalid_1's ndcg@20: 0.609125\n",
      "[50]\ttraining's ndcg@20: 0.752307\tvalid_1's ndcg@20: 0.609826\n",
      "[60]\ttraining's ndcg@20: 0.753446\tvalid_1's ndcg@20: 0.610302\n",
      "[70]\ttraining's ndcg@20: 0.754539\tvalid_1's ndcg@20: 0.610757\n",
      "[80]\ttraining's ndcg@20: 0.755557\tvalid_1's ndcg@20: 0.610914\n",
      "[90]\ttraining's ndcg@20: 0.756559\tvalid_1's ndcg@20: 0.611134\n",
      "[100]\ttraining's ndcg@20: 0.757536\tvalid_1's ndcg@20: 0.611306\n",
      "[110]\ttraining's ndcg@20: 0.758425\tvalid_1's ndcg@20: 0.611422\n",
      "[120]\ttraining's ndcg@20: 0.759292\tvalid_1's ndcg@20: 0.61141\n",
      "[130]\ttraining's ndcg@20: 0.760092\tvalid_1's ndcg@20: 0.611479\n",
      "[140]\ttraining's ndcg@20: 0.760872\tvalid_1's ndcg@20: 0.611536\n",
      "[150]\ttraining's ndcg@20: 0.761603\tvalid_1's ndcg@20: 0.611564\n",
      "[160]\ttraining's ndcg@20: 0.762261\tvalid_1's ndcg@20: 0.611487\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's ndcg@20: 0.761147\tvalid_1's ndcg@20: 0.611575\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 0 clicks recall = 0.8852307119243842\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1/5....\n",
      "before mean: 0.008131663513274904\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (34363120, 168)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.303455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36132\n",
      "[LightGBM] [Info] Number of data points in the train set: 34363120, number of used features: 168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.746756\tvalid_1's ndcg@20: 0.607599\n",
      "[20]\ttraining's ndcg@20: 0.748443\tvalid_1's ndcg@20: 0.609327\n",
      "[30]\ttraining's ndcg@20: 0.749762\tvalid_1's ndcg@20: 0.610277\n",
      "[40]\ttraining's ndcg@20: 0.751026\tvalid_1's ndcg@20: 0.611145\n",
      "[50]\ttraining's ndcg@20: 0.752165\tvalid_1's ndcg@20: 0.611798\n",
      "[60]\ttraining's ndcg@20: 0.753237\tvalid_1's ndcg@20: 0.612244\n",
      "[70]\ttraining's ndcg@20: 0.754295\tvalid_1's ndcg@20: 0.612572\n",
      "[80]\ttraining's ndcg@20: 0.75528\tvalid_1's ndcg@20: 0.612822\n",
      "[90]\ttraining's ndcg@20: 0.756338\tvalid_1's ndcg@20: 0.613135\n",
      "[100]\ttraining's ndcg@20: 0.757323\tvalid_1's ndcg@20: 0.613254\n",
      "[110]\ttraining's ndcg@20: 0.758255\tvalid_1's ndcg@20: 0.613274\n",
      "[120]\ttraining's ndcg@20: 0.759163\tvalid_1's ndcg@20: 0.613337\n",
      "[130]\ttraining's ndcg@20: 0.760037\tvalid_1's ndcg@20: 0.613351\n",
      "[140]\ttraining's ndcg@20: 0.760737\tvalid_1's ndcg@20: 0.613384\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's ndcg@20: 0.759779\tvalid_1's ndcg@20: 0.613408\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 1 clicks recall = 0.8857196070214648\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2/5....\n",
      "before mean: 0.008134454971878551\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (34363120, 168)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.297541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36146\n",
      "[LightGBM] [Info] Number of data points in the train set: 34363120, number of used features: 168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.747033\tvalid_1's ndcg@20: 0.60553\n",
      "[20]\ttraining's ndcg@20: 0.748912\tvalid_1's ndcg@20: 0.607294\n",
      "[30]\ttraining's ndcg@20: 0.75024\tvalid_1's ndcg@20: 0.60822\n",
      "[40]\ttraining's ndcg@20: 0.751439\tvalid_1's ndcg@20: 0.609183\n",
      "[50]\ttraining's ndcg@20: 0.752601\tvalid_1's ndcg@20: 0.609964\n",
      "[60]\ttraining's ndcg@20: 0.753749\tvalid_1's ndcg@20: 0.610362\n",
      "[70]\ttraining's ndcg@20: 0.754836\tvalid_1's ndcg@20: 0.610612\n",
      "[80]\ttraining's ndcg@20: 0.755831\tvalid_1's ndcg@20: 0.610867\n",
      "[90]\ttraining's ndcg@20: 0.756897\tvalid_1's ndcg@20: 0.61105\n",
      "[100]\ttraining's ndcg@20: 0.757879\tvalid_1's ndcg@20: 0.61126\n",
      "[110]\ttraining's ndcg@20: 0.758818\tvalid_1's ndcg@20: 0.611401\n",
      "[120]\ttraining's ndcg@20: 0.759659\tvalid_1's ndcg@20: 0.611498\n",
      "[130]\ttraining's ndcg@20: 0.760518\tvalid_1's ndcg@20: 0.611466\n",
      "[140]\ttraining's ndcg@20: 0.761188\tvalid_1's ndcg@20: 0.611519\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's ndcg@20: 0.75983\tvalid_1's ndcg@20: 0.611537\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 2 clicks recall = 0.8846300693765423\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3/5....\n",
      "before mean: 0.0081348555092286\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n",
      "remove id from features....\n",
      "x_train shape: (34363160, 168)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 11.668457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36154\n",
      "[LightGBM] [Info] Number of data points in the train set: 34363160, number of used features: 168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.746999\tvalid_1's ndcg@20: 0.606001\n",
      "[20]\ttraining's ndcg@20: 0.748715\tvalid_1's ndcg@20: 0.607357\n",
      "[30]\ttraining's ndcg@20: 0.750071\tvalid_1's ndcg@20: 0.608643\n",
      "[40]\ttraining's ndcg@20: 0.75131\tvalid_1's ndcg@20: 0.609474\n",
      "[50]\ttraining's ndcg@20: 0.752515\tvalid_1's ndcg@20: 0.610247\n",
      "[60]\ttraining's ndcg@20: 0.753552\tvalid_1's ndcg@20: 0.610729\n",
      "[70]\ttraining's ndcg@20: 0.754588\tvalid_1's ndcg@20: 0.611099\n",
      "[80]\ttraining's ndcg@20: 0.755634\tvalid_1's ndcg@20: 0.61135\n",
      "[90]\ttraining's ndcg@20: 0.756646\tvalid_1's ndcg@20: 0.611618\n",
      "[100]\ttraining's ndcg@20: 0.7576\tvalid_1's ndcg@20: 0.611742\n",
      "[110]\ttraining's ndcg@20: 0.758534\tvalid_1's ndcg@20: 0.611887\n",
      "[120]\ttraining's ndcg@20: 0.759366\tvalid_1's ndcg@20: 0.611949\n",
      "[130]\ttraining's ndcg@20: 0.760146\tvalid_1's ndcg@20: 0.611953\n",
      "[140]\ttraining's ndcg@20: 0.760783\tvalid_1's ndcg@20: 0.611998\n",
      "[150]\ttraining's ndcg@20: 0.761371\tvalid_1's ndcg@20: 0.612012\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's ndcg@20: 0.760411\tvalid_1's ndcg@20: 0.612044\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 3 clicks recall = 0.8851975843813585\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4/5....\n",
      "before mean: 0.008133307939043898\n",
      "under sampling....... 1 / 5\n",
      "under sampling....... 2 / 5\n",
      "under sampling....... 3 / 5\n",
      "under sampling....... 4 / 5\n",
      "under sampling....... 5 / 5\n",
      "under sampling end\n",
      "post proccess1\n",
      "after mean: 0.025\n",
      "add session features....\n",
      "add aid features....\n",
      "add interactive features....\n",
      "remove features....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove id from features....\n",
      "x_train shape: (34363160, 168)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 20.205207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 36157\n",
      "[LightGBM] [Info] Number of data points in the train set: 34363160, number of used features: 168\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[10]\ttraining's ndcg@20: 0.74672\tvalid_1's ndcg@20: 0.607019\n",
      "[20]\ttraining's ndcg@20: 0.748329\tvalid_1's ndcg@20: 0.608752\n",
      "[30]\ttraining's ndcg@20: 0.749605\tvalid_1's ndcg@20: 0.609763\n",
      "[40]\ttraining's ndcg@20: 0.75084\tvalid_1's ndcg@20: 0.610572\n",
      "[50]\ttraining's ndcg@20: 0.751997\tvalid_1's ndcg@20: 0.611269\n",
      "[60]\ttraining's ndcg@20: 0.753084\tvalid_1's ndcg@20: 0.611779\n",
      "[70]\ttraining's ndcg@20: 0.754122\tvalid_1's ndcg@20: 0.61208\n",
      "[80]\ttraining's ndcg@20: 0.755119\tvalid_1's ndcg@20: 0.61236\n",
      "[90]\ttraining's ndcg@20: 0.756208\tvalid_1's ndcg@20: 0.612526\n",
      "[100]\ttraining's ndcg@20: 0.757183\tvalid_1's ndcg@20: 0.612661\n",
      "[110]\ttraining's ndcg@20: 0.758101\tvalid_1's ndcg@20: 0.612718\n",
      "[120]\ttraining's ndcg@20: 0.758936\tvalid_1's ndcg@20: 0.612772\n",
      "[130]\ttraining's ndcg@20: 0.759685\tvalid_1's ndcg@20: 0.612793\n",
      "[140]\ttraining's ndcg@20: 0.760383\tvalid_1's ndcg@20: 0.612813\n",
      "[150]\ttraining's ndcg@20: 0.761075\tvalid_1's ndcg@20: 0.61276\n",
      "[160]\ttraining's ndcg@20: 0.761671\tvalid_1's ndcg@20: 0.612766\n",
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's ndcg@20: 0.760456\tvalid_1's ndcg@20: 0.612822\n",
      "train pred i= 0\n",
      "train pred i= 1\n",
      "train pred i= 2\n",
      "train pred i= 3\n",
      "train pred i= 4\n",
      "fold 4 clicks recall = 0.8857702927331226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "session = train['session']\n",
    "unique_session = session.unique()\n",
    "\n",
    "N_splits = 5\n",
    "kfold = KFold(n_splits = N_splits, shuffle = True, random_state = 42)\n",
    "for fold, (trn_group_ind, val_group_ind) in enumerate(kfold.split(unique_session)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold}/{N_splits}....')\n",
    "    # session単位で分割してKFoldする\n",
    "    tr_groups, va_groups = unique_session[trn_group_ind], unique_session[val_group_ind]\n",
    "    is_tr, is_va = session.isin(tr_groups), session.isin(va_groups)\n",
    "    del tr_groups, va_groups\n",
    "    gc.collect()\n",
    "    # is_ir, is_va=Trueのindexを取得\n",
    "    trn_ind, val_ind = is_tr[is_tr].index, is_va[is_va].index\n",
    "    del is_tr, is_va\n",
    "    gc.collect()\n",
    "\n",
    "    y_train, y_val = train[target].iloc[trn_ind], train[target].iloc[val_ind]\n",
    "    train_tmp = train.drop(IGNORE_COL_TARGET , axis=1)\n",
    "    x_train, x_val = train_tmp.iloc[trn_ind], train_tmp.iloc[val_ind]\n",
    "    del train_tmp\n",
    "    gc.collect()\n",
    "    # under sampling\n",
    "    x_train, y_train = negative_sampling(x_train, y_train, pos_neg_ratio)\n",
    "\n",
    "    # queryの準備, sessionごとにsortする, lightGBMでranking metricsを使うときに必要\n",
    "    query_list_train = x_train['session'].value_counts()\n",
    "    #x_train = x_train.drop('session' , axis=1)\n",
    "    query_list_train = query_list_train.sort_index()\n",
    "\n",
    "    query_list_valid = x_val['session'].value_counts()\n",
    "    #x_val = x_val.drop('session' , axis=1)\n",
    "    query_list_valid = query_list_valid.sort_index()\n",
    "\n",
    "    # memory節約のため, under sampling後にfeature追加\n",
    "    session_path = f'{base_path}/output/otto/valid_session_features.parquet'\n",
    "    aid_path = f'{base_path}/output/otto/valid_aid_features.parquet'\n",
    "\n",
    "\n",
    "    print('add session features....')\n",
    "    x_train, x_val = fe.join_session_features(x_train, session_path), fe.join_session_features(x_val, session_path)\n",
    "    print('add aid features....')\n",
    "    x_train, x_val = fe.join_aid_features(x_train, aid_path), fe.join_aid_features(x_val, aid_path)\n",
    "    print('add interactive features....')\n",
    "    x_train, x_val = fe.join_interactive_features(x_train), fe.join_interactive_features(x_val)\n",
    "    print('remove features....')\n",
    "    x_train, x_val = remove_features(x_train),  remove_features(x_val)\n",
    "    print('remove id from features....')\n",
    "    x_train, x_val = x_train.drop(IGNORE_COL_ID, axis=1), x_val.drop(IGNORE_COL_ID, axis=1)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, group=query_list_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, group=query_list_valid)\n",
    "\n",
    "    del x_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        #num_boost_round = 100,\n",
    "        num_boost_round = 2000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 20,\n",
    "        verbose_eval = 10,\n",
    "        )\n",
    "    del lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'{base_path}/otto/otto_lgbm_fold{fold}_{TYPE_MODE}.pkl')\n",
    "    # Predict validation\n",
    "    # でかいので分割してpredict\n",
    "    Nrow = x_val.shape[0]\n",
    "    Ndiv = 5\n",
    "    n = int(Nrow // Ndiv) + 1\n",
    "    x_val_list = []\n",
    "    for i in range(Ndiv):\n",
    "        tmp = x_val.iloc[i*n : (i+1)*n, :]\n",
    "        x_val_list.append(tmp)\n",
    "    del x_val\n",
    "    gc.collect()\n",
    "\n",
    "    val_pred_list = []\n",
    "    for i, v in enumerate(x_val_list):\n",
    "        print('train pred i=', i)\n",
    "        tmp = model.predict(v)\n",
    "        val_pred_list.append(tmp)\n",
    "    del x_val_list\n",
    "    gc.collect()\n",
    "    val_pred = np.concatenate(val_pred_list)\n",
    "    del val_pred_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Add to out of folds array\n",
    "    # CVを終えれば全部のindexが1回ずつ計算されることになる\n",
    "    oof_predictions[val_ind] = val_pred\n",
    "\n",
    "    # 不要になった時点でモデル削除\n",
    "    del model, y_val\n",
    "    gc.collect()\n",
    "\n",
    "    # tmp recall for each fold\n",
    "    df = pd.DataFrame(val_pred, columns=[\"score\"])\n",
    "    tmp = train[['session', 'aid']].iloc[val_ind].reset_index(drop=True)\n",
    "    pred_df = pd.concat([tmp, df], axis=1)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "    pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "    pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "    pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "    pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "    pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "    pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "    pred_df = pred_df.drop(['aid'],axis=1)\n",
    "\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "    test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "    # foldごとのreallなのでinnter\n",
    "    test_labels = test_labels.merge(sub, how='inner', on=['session']) \n",
    "    test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    print(f'fold {fold} {TYPE_MODE} recall =',recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x0DTGgkdT2si"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 295362 217742 1544564 1383767 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098534_clicks</td>\n",
       "      <td>223062 908024 1342293 1607945 1300062 1649004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098535_clicks</td>\n",
       "      <td>745365 803918 1750442 767201 896972 85930 9076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098537_clicks</td>\n",
       "      <td>358965 336024 1409748 1723620 294268 1092681 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098538_clicks</td>\n",
       "      <td>1263747 1550143 703265 1711586 717871 351587 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073843</th>\n",
       "      <td>12899774_clicks</td>\n",
       "      <td>33035 1539309 270852 95488 1226691 819288 7719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073844</th>\n",
       "      <td>12899775_clicks</td>\n",
       "      <td>1743151 1760714 1255910 1163166 1022572 123968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073845</th>\n",
       "      <td>12899776_clicks</td>\n",
       "      <td>548599 1440959 1401030 1144446 1150130 1607333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073846</th>\n",
       "      <td>12899777_clicks</td>\n",
       "      <td>384045 1308634 1688215 395762 703474 1486067 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073847</th>\n",
       "      <td>12899778_clicks</td>\n",
       "      <td>561560 1167224 1175618 32070 13942 566042 5705...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073848 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098529_clicks  1105029 459126 295362 217742 1544564 1383767 1...\n",
       "1        11098534_clicks  223062 908024 1342293 1607945 1300062 1649004 ...\n",
       "2        11098535_clicks  745365 803918 1750442 767201 896972 85930 9076...\n",
       "3        11098537_clicks  358965 336024 1409748 1723620 294268 1092681 1...\n",
       "4        11098538_clicks  1263747 1550143 703265 1711586 717871 351587 1...\n",
       "...                  ...                                                ...\n",
       "1073843  12899774_clicks  33035 1539309 270852 95488 1226691 819288 7719...\n",
       "1073844  12899775_clicks  1743151 1760714 1255910 1163166 1022572 123968...\n",
       "1073845  12899776_clicks  548599 1440959 1401030 1144446 1150130 1607333...\n",
       "1073846  12899777_clicks  384045 1308634 1688215 395762 703474 1486067 2...\n",
       "1073847  12899778_clicks  561560 1167224 1175618 32070 13942 566042 5705...\n",
       "\n",
       "[1073848 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(oof_predictions, columns=[\"score\"])\n",
    "#df.to_csv(f'{base_path}/otto/oof_lgbm_{TYPE_MODE}.csv', index = False)\n",
    "\n",
    "pred_df = pd.concat([train[['session', 'aid']], df], axis=1)\n",
    "pred_df['session_type'] = pred_df['session'].apply(lambda x: str(x) + f'_{TYPE_MODE}')\n",
    "pred_df = pred_df.sort_values(['session_type','score'],ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "pred_df['n'] = pred_df.groupby('session_type').cumcount()\n",
    "pred_df = pred_df.loc[pred_df.n<20].drop(['n','score','session'],axis=1)\n",
    "pred_df['aid'] = pred_df['aid'].astype('int32')\n",
    "pred_df = pred_df.groupby('session_type')['aid'].apply(list).reset_index()\n",
    "pred_df['labels'] = pred_df['aid'].map(lambda x: ''.join(str(x)[1:-1].split(',')))\n",
    "pred_df = pred_df.drop(['aid'],axis=1)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6Jd5N7_5V44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5415377885019601\n"
     ]
    }
   ],
   "source": [
    "sub = pred_df.loc[pred_df.session_type.str.contains(TYPE_MODE)].copy()\n",
    "sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "test_labels = pd.read_parquet(f'{base_path}/input/otto/otto-validation/test_labels.parquet')\n",
    "test_labels = test_labels.loc[test_labels['type']==TYPE_MODE]\n",
    "test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "test_labels['labels'] = test_labels['labels'].fillna('[]')\n",
    "test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "print(f'{TYPE_MODE} recall =',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZiU5CW6NeKKS"
   },
   "outputs": [],
   "source": [
    "# click total: 1,755,534\n",
    "# 0.52なら912,877の正解が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LskmzPtiPuzJ"
   },
   "outputs": [],
   "source": [
    "# ranker model, fold0を factor=1.111369 で割ればそれっぽい値が出る, each foldは session inner joinなので高めに出る\n",
    "# num=100, lr=0.1, no feature add, valid_1's ndcg@50: 0.843586, fold 0 orders recall = 0.7263385039885385, orders recall = 0.6535526311589739\n",
    "# add aid feature, valid_1's ndcg@50: 0.84962, fold 0 orders recall = 0.7305304490864389, (orders recall = 0.657324?)\n",
    "# add aid + session feature, valid_1's ndcg@50: 0.849762, fold 0 orders recall = 0.7302651361055592, orders recall = 0.6575806806829172\n",
    "# all valid_1's ndcg@50: 0.848664, fold 0 orders recall = 0.730990324919964, orders recall = 0.6579892308723504\n",
    "\n",
    "\n",
    "# hypter paramやりなおし、regularization param大幅変更 num=100, lr=0.1\n",
    "# no add: valid_1's ndcg@20: 0.835401, fold 0 orders recall = 0.7261793162000106, orders recall = 0.6535877409408783  -> order,carts差し替えPB = 0.581\n",
    "#                                                                                 carts recall = 0.41837386076234817\n",
    "# sessionのみ: valid_1's ndcg@20: 0.836164, fold 0 orders recall = 0.7268514424182394, orders recall = 0.6545069788671031 -> order,carts差し替えPB = 0.583\n",
    "#                                                                                      carts recall = 0.4196106730132077\n",
    "# aidのみ: valid_1's ndcg@20: 0.842205, fold 0 orders recall = 0.7302828236376179, orders recall = 0.6575838724812721 -> order,carts差し替えPB = 0.586 (55, 1/19)\n",
    "#                                                                                  carts recall = 0.42261163401459195                              \n",
    "# aid+session: valid_1's ndcg@20: 0.843169, fold 0 orders recall = 0.7309018872596706, orders recall = 0.6582030813621319 -> order,carts差し替えPB = 0.587 (54, 1/19)\n",
    "#                                                                                      carts recall = 0.42347896378377814\n",
    "# all: valid_1's ndcg@20: 0.843514, fold 0 orders recall = 0.731184887772609, orders recall = 0.6584137400535583 -> order,carts差し替えPB = 0.586 下がったけどブレ？\n",
    "#                                                                             carts recall = 0.42349804503870025\n",
    "# num=1000, lr=0.05 [281] valid_1's ndcg@20: 0.844737, fold 0 orders recall = 0.7315386384137821, orders recall = 0.6586627003252442 -> PB = 0.587 (53, 1/19)\n",
    "#                                                                                                 carts recall = 0.4242057861303562\n",
    "#                                                                                                 clicks recall = 0.536971\n",
    "#c andidate add, mean 60 -> 120\n",
    "# lr=0.1, [100] valid_1's ndcg@20: 0.832289 fold 0 orders recall = 0.7260696029689797, orders recall = 0.6619853624127442\n",
    "# lr=0.05,[172] valid_1's ndcg@20: 0.833279,fold 0 orders recall = 0.7268398571528605, orders recall = 0.6622981586515291\n",
    "#                                                                                      carts recall = 0.4296682290166909\n",
    "#                                                                                      clicks recall = 0.54153778850196010\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NS3YOf-UgmrY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
